import { configStore } from "./config"
import {
  MCPTool,
  MCPToolCall,
  LLMToolCallResponse,
  MCPToolResult,
} from "./mcp-service"
import { AgentProgressStep, AgentProgressUpdate, SessionProfileSnapshot, AgentMemory } from "../shared/types"
import { diagnosticsService } from "./diagnostics"

import { makeLLMCallWithFetch, makeTextCompletionWithFetch, verifyCompletionWithFetch, RetryProgressCallback, makeLLMCallWithStreaming, StreamingCallback } from "./llm-fetch"
import { constructSystemPrompt } from "./system-prompts"
import { state, agentSessionStateManager } from "./state"
import { isDebugLLM, logLLM, isDebugTools, logTools } from "./debug"
import { shrinkMessagesForLLM, estimateTokensFromMessages } from "./context-budget"
import { emitAgentProgress } from "./emit-agent-progress"
import { agentSessionTracker } from "./agent-session-tracker"
import { conversationService } from "./conversation-service"
import { getCurrentPresetName } from "../shared"
import {
  createAgentTrace,
  endAgentTrace,
  isLangfuseEnabled,
  flushLangfuse,
} from "./langfuse-service"
import {
  isSummarizationEnabled,
  shouldSummarizeStep,
  summarizeAgentStep,
  summarizationService,
  type SummarizationInput,
} from "./summarization-service"
import { memoryService } from "./memory-service"

/**
 * Clean error message by removing stack traces and noise
 */
function cleanErrorMessage(errorText: string): string {
  // Remove stack traces (lines starting with "at " after an error)
  const lines = errorText.split('\n')
  const cleanedLines: string[] = []

  for (const line of lines) {
    const trimmed = line.trim()
    // Skip stack trace lines
    if (trimmed.startsWith('at ')) continue
    // Skip file path lines
    if (trimmed.match(/^\s*at\s+.*\.(js|ts|mjs):\d+/)) continue
    // Skip empty lines in stack traces
    if (cleanedLines.length > 0 && trimmed === '' && lines.indexOf(line) > 0) {
      const prevLine = lines[lines.indexOf(line) - 1]?.trim()
      if (prevLine?.startsWith('at ')) continue
    }
    cleanedLines.push(line)
  }

  let cleaned = cleanedLines.join('\n').trim()

  // Remove duplicate error class names (e.g., "CodeExecutionTimeoutError: Code execution timed out")
  cleaned = cleaned.replace(/(\w+Error):\s*\1:/g, '$1:')

  // Truncate if still too long
  if (cleaned.length > 500) {
    cleaned = cleaned.substring(0, 500) + '...'
  }

  return cleaned
}

/**
 * Analyze tool errors and categorize them
 */
function analyzeToolErrors(toolResults: MCPToolResult[]): {
  errorTypes: string[]
} {
  const errorTypes: string[] = []
  const errorMessages = toolResults
    .filter((r) => r.isError)
    .map((r) => r.content.map((c) => c.text).join(" ").toLowerCase())
    .join(" ")

  // Categorize error types
  if (errorMessages.includes("timeout")) {
    errorTypes.push("timeout")
  }
  if (errorMessages.includes("connection") || errorMessages.includes("network")) {
    errorTypes.push("connectivity")
  }
  if (errorMessages.includes("permission") || errorMessages.includes("access") || errorMessages.includes("denied")) {
    errorTypes.push("permissions")
  }
  if (errorMessages.includes("not found") || errorMessages.includes("does not exist") || errorMessages.includes("missing")) {
    errorTypes.push("not_found")
  }
  if (errorMessages.includes("invalid") || errorMessages.includes("expected")) {
    errorTypes.push("invalid_params")
  }

  return { errorTypes }
}

export async function postProcessTranscript(transcript: string) {
  const config = configStore.get()

  if (
    !config.transcriptPostProcessingEnabled ||
    !config.transcriptPostProcessingPrompt
  ) {
    return transcript
  }

  let prompt = config.transcriptPostProcessingPrompt

  if (prompt.includes("{transcript}")) {
    prompt = prompt.replaceAll("{transcript}", transcript)
  } else {
    prompt = prompt + "\n\n" + transcript
  }

  const chatProviderId = config.transcriptPostProcessingProviderId

  try {
    const result = await makeTextCompletionWithFetch(prompt, chatProviderId)
    return result
  } catch (error) {
    throw error
  }
}

export async function processTranscriptWithTools(
  transcript: string,
  availableTools: MCPTool[],
): Promise<LLMToolCallResponse> {
  const config = configStore.get()

  const uniqueAvailableTools = availableTools.filter(
    (tool, index, self) =>
      index === self.findIndex((t) => t.name === tool.name),
  )

  const userGuidelines = config.mcpToolsSystemPrompt
  // Load enabled agent skills instructions for non-agent mode too
  // Use the current profile's skills config
  const { skillsService } = await import("./skills-service")
  const { profileService } = await import("./profile-service")
  const currentProfileId = config.mcpCurrentProfileId
  const enabledSkillIds = currentProfileId
    ? profileService.getEnabledSkillIdsForProfile(currentProfileId)
    : []
  const skillsInstructions = skillsService.getEnabledSkillsInstructionsForProfile(enabledSkillIds)

  // Load memories for context (works independently of dual-model summarization)
  // Memories are filtered by current profile
  // Only load if both memoriesEnabled (system-wide) and dualModelInjectMemories are true
  let relevantMemories: AgentMemory[] = []
  if (config.memoriesEnabled !== false && config.dualModelInjectMemories) {
    const currentProfileId = config.mcpCurrentProfileId
    const allMemories = currentProfileId
      ? await memoryService.getMemoriesByProfile(currentProfileId)
      : await memoryService.getAllMemories()
    // Sort by importance first (critical > high > medium > low), then by recency, before capping
    const importanceOrder: Record<string, number> = { critical: 0, high: 1, medium: 2, low: 3 }
    const sortedMemories = [...allMemories].sort((a, b) => {
      const impDiff = importanceOrder[a.importance] - importanceOrder[b.importance]
      if (impDiff !== 0) return impDiff
      return b.createdAt - a.createdAt // More recent first as tiebreaker
    })
    relevantMemories = sortedMemories.slice(0, 10)
    logLLM(`[processTranscriptWithLLM] Loaded ${relevantMemories.length} memories for context (profile: ${currentProfileId || 'global'})`)
  }

  const systemPrompt = constructSystemPrompt(
    uniqueAvailableTools,
    userGuidelines,
    false,
    undefined,
    config.mcpCustomSystemPrompt,
    skillsInstructions,
    undefined, // personaProperties - not used in non-agent mode
    relevantMemories,
  )

  const messages = [
    {
      role: "system",
      content: systemPrompt,
    },
    {
      role: "user",
      content: transcript,
    },
  ]

  const { messages: shrunkMessages } = await shrinkMessagesForLLM({
    messages,
    availableTools: uniqueAvailableTools,
    isAgentMode: false,
  })

  const chatProviderId = config.mcpToolsProviderId

  try {
    // Pass tools for native AI SDK tool calling
    const result = await makeLLMCallWithFetch(shrunkMessages, chatProviderId, undefined, undefined, uniqueAvailableTools)
    return result
  } catch (error) {
    throw error
  }
}

export interface AgentModeResponse {
  content: string
  conversationHistory: Array<{
    role: "user" | "assistant" | "tool"
    content: string
    toolCalls?: MCPToolCall[]
    toolResults?: MCPToolResult[]
  }>
  totalIterations: number
}

function createProgressStep(
  type: AgentProgressStep["type"],
  title: string,
  description?: string,
  status: AgentProgressStep["status"] = "pending",
): AgentProgressStep {
  return {
    id: `step_${Date.now()}_${Math.random().toString(36).substring(2, 11)}`,
    type,
    title,
    description,
    status,
    timestamp: Date.now(),
  }
}

/**
 * Result from a single tool execution including metadata for progress tracking
 */
interface ToolExecutionResult {
  toolCall: MCPToolCall
  result: MCPToolResult
  retryCount: number
  cancelledByKill: boolean
}

/**
 * Execute a single tool call with retry logic and kill switch support
 * This helper is used by both sequential and parallel execution modes
 */
async function executeToolWithRetries(
  toolCall: MCPToolCall,
  executeToolCall: (toolCall: MCPToolCall, onProgress?: (message: string) => void) => Promise<MCPToolResult>,
  currentSessionId: string,
  onToolProgress: (message: string) => void,
  maxRetries: number = 2,
): Promise<ToolExecutionResult> {
  // Check for stop signal before starting
  if (agentSessionStateManager.shouldStopSession(currentSessionId)) {
    return {
      toolCall,
      result: {
        content: [{ type: "text", text: "Tool execution cancelled by emergency kill switch" }],
        isError: true,
      },
      retryCount: 0,
      cancelledByKill: true,
    }
  }

  // Execute tool with cancel-aware race so kill switch can stop mid-tool
  let cancelledByKill = false
  let cancelInterval: ReturnType<typeof setInterval> | null = null
  const stopPromise: Promise<MCPToolResult> = new Promise((resolve) => {
    cancelInterval = setInterval(() => {
      if (agentSessionStateManager.shouldStopSession(currentSessionId)) {
        cancelledByKill = true
        if (cancelInterval) clearInterval(cancelInterval)
        resolve({
          content: [{ type: "text", text: "Tool execution cancelled by emergency kill switch" }],
          isError: true,
        })
      }
    }, 100)
  })

  const execPromise = executeToolCall(toolCall, onToolProgress)
  let result = (await Promise.race([
    execPromise,
    stopPromise,
  ])) as MCPToolResult
  // Avoid unhandled rejection if the tool promise rejects after we already stopped
  if (cancelledByKill) {
    execPromise.catch(() => { /* swallow after kill switch */ })
  }
  if (cancelInterval) clearInterval(cancelInterval)

  if (cancelledByKill) {
    return {
      toolCall,
      result,
      retryCount: 0,
      cancelledByKill: true,
    }
  }

  // Enhanced retry logic for specific error types
  let retryCount = 0
  while (result.isError && retryCount < maxRetries) {
    // Check kill switch before retrying
    if (agentSessionStateManager.shouldStopSession(currentSessionId)) {
      return {
        toolCall,
        result: {
          content: [{ type: "text", text: "Tool execution cancelled by emergency kill switch" }],
          isError: true,
        },
        retryCount,
        cancelledByKill: true,
      }
    }

    const errorText = result.content
      .map((c) => c.text)
      .join(" ")
      .toLowerCase()

    // Check if this is a retryable error
    const isRetryableError =
      errorText.includes("timeout") ||
      errorText.includes("connection") ||
      errorText.includes("network") ||
      errorText.includes("temporary") ||
      errorText.includes("busy")

    if (isRetryableError) {
      retryCount++

      // Wait before retry (exponential backoff)
      await new Promise((resolve) =>
        setTimeout(resolve, Math.pow(2, retryCount) * 1000),
      )

      result = await executeToolCall(toolCall, onToolProgress)
    } else {
      break // Don't retry non-transient errors
    }
  }

  return {
    toolCall,
    result,
    retryCount,
    cancelledByKill: false,
  }
}

export async function processTranscriptWithAgentMode(
  transcript: string,
  availableTools: MCPTool[],
  executeToolCall: (toolCall: MCPToolCall, onProgress?: (message: string) => void) => Promise<MCPToolResult>,
  maxIterations: number = 10,
  previousConversationHistory?: Array<{
    role: "user" | "assistant" | "tool"
    content: string
    toolCalls?: MCPToolCall[]
    toolResults?: MCPToolResult[]
  }>,
  conversationId?: string, // Conversation ID for linking to conversation history
  sessionId?: string, // Session ID for progress routing and isolation
  onProgress?: (update: AgentProgressUpdate) => void, // Optional callback for external progress consumers (e.g., SSE)
  profileSnapshot?: SessionProfileSnapshot, // Profile snapshot for session isolation
): Promise<AgentModeResponse> {
  const config = configStore.get()

  // Store IDs for use in progress updates
  const currentConversationId = conversationId
  const currentSessionId =
    sessionId || `session_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`
  // Number of messages in the conversation history that predate this agent session.
  // Used by the UI to show only this session's messages while still saving full history.
  // When continuing a conversation, we set this to 0 so the UI shows the full history.
  // The user explicitly wants to see the previous context when they click "Continue".
  const sessionStartIndex = 0

  // For session isolation: prefer the stored snapshot over the passed-in one
  // This ensures that when reusing an existing sessionId, we maintain the original profile settings
  // and don't allow mid-session profile changes to affect the session
  const storedSnapshot = sessionId ? agentSessionStateManager.getSessionProfileSnapshot(sessionId) : undefined
  const effectiveProfileSnapshot = storedSnapshot ?? profileSnapshot

  // Create session state for this agent run with profile snapshot for isolation
  // Note: createSession is a no-op if the session already exists, so this is safe for resumed sessions
  agentSessionStateManager.createSession(currentSessionId, effectiveProfileSnapshot)

  // Track step summaries for dual-model mode
  const stepSummaries: import("../shared/types").AgentStepSummary[] = []

  // Create Langfuse trace for this agent session if enabled
  // - traceId: unique ID for this trace (our agent session ID)
  // - sessionId: groups traces together in Langfuse (our conversation ID)
  if (isLangfuseEnabled()) {
    createAgentTrace(currentSessionId, {
      name: "Agent Session",
      sessionId: currentConversationId,  // Groups all agent sessions in this conversation
      metadata: {
        maxIterations,
        hasHistory: !!previousConversationHistory?.length,
        profileId: effectiveProfileSnapshot?.profileId,
        profileName: effectiveProfileSnapshot?.profileName,
      },
      input: transcript,
      tags: effectiveProfileSnapshot?.profileName
        ? [`profile:${effectiveProfileSnapshot.profileName}`]
        : undefined,
    })
  }

  // Declare variables that need to be accessible in the finally block for Langfuse tracing
  let iteration = 0
  let finalContent = ""
  let wasAborted = false // Track if agent was aborted for observability
  let toolsExecutedInSession = false // Track if ANY tools were executed, survives context shrinking

  try {
  // Track context usage info for progress display
  // Declared here so emit() can access it
  let contextInfoRef: { estTokens: number; maxTokens: number } | undefined = undefined

  // Get model info for progress display
  const providerId = config.mcpToolsProviderId || "nemotron"
  const modelName = config.mcpToolsNemotronModel || "nvidia/llama-3.1-nemotron-70b-instruct"
  const providerDisplayName = "Nemotron"
  const modelInfoRef = { provider: providerDisplayName, model: modelName }

  // Create bound emitter that always includes sessionId, conversationId, snooze state, sessionStartIndex, conversationTitle, and contextInfo
  const emit = (
    update: Omit<AgentProgressUpdate, 'sessionId' | 'conversationId' | 'isSnoozed' | 'conversationTitle'>,
  ) => {
    const isSnoozed = agentSessionTracker.isSessionSnoozed(currentSessionId)
    const session = agentSessionTracker.getSession(currentSessionId)
    const conversationTitle = session?.conversationTitle
    const profileName = session?.profileSnapshot?.profileName

    const fullUpdate: AgentProgressUpdate = {
      ...update,
      sessionId: currentSessionId,
      conversationId: currentConversationId,
      conversationTitle,
      isSnoozed,
      sessionStartIndex,
      // Always include current context info if available
      contextInfo: update.contextInfo ?? contextInfoRef,
      // Always include model info
      modelInfo: modelInfoRef,
      // Include profile name from session snapshot for UI display
      profileName,
      // Dual-model summarization data (from service - single source of truth)
      stepSummaries: summarizationService.getSummaries(currentSessionId),
      latestSummary: summarizationService.getLatestSummary(currentSessionId),
    }

    // Fire and forget - don't await, but catch errors
    emitAgentProgress(fullUpdate).catch(err => {
      logLLM("[emit] Failed to emit agent progress:", err)
    })

    // Also call external progress callback if provided (for SSE streaming, etc.)
    if (onProgress) {
      try {
        onProgress(fullUpdate)
      } catch (err) {
        logLLM("[emit] Failed to call onProgress callback:", err)
      }
    }
  }

  // Helper function to save a message incrementally to the conversation
  // This ensures messages are persisted even if the agent crashes or is stopped
  const saveMessageIncremental = async (
    role: "user" | "assistant" | "tool",
    content: string,
    toolCalls?: MCPToolCall[],
    toolResults?: MCPToolResult[]
  ) => {
    if (!currentConversationId) {
      return // No conversation to save to
    }

    try {
      // Convert toolResults from MCPToolResult format to stored format
      const convertedToolResults = toolResults?.map(tr => ({
        success: !tr.isError,
        content: Array.isArray(tr.content)
          ? tr.content.map(c => c.text).join("\n")
          : String(tr.content || ""),
        error: tr.isError
          ? (Array.isArray(tr.content) ? tr.content.map(c => c.text).join("\n") : String(tr.content || ""))
          : undefined
      }))

      await conversationService.addMessageToConversation(
        currentConversationId,
        content,
        role,
        toolCalls,
        convertedToolResults
      )

      if (isDebugLLM()) {
        logLLM("ðŸ’¾ Saved message incrementally", {
          conversationId: currentConversationId,
          role,
          contentLength: content.length,
          hasToolCalls: !!toolCalls,
          hasToolResults: !!toolResults
        })
      }
    } catch (error) {
      // Log but don't throw - persistence failures shouldn't crash the agent
      logLLM("[saveMessageIncremental] Failed to save message:", error)
      diagnosticsService.logWarning("llm", "Failed to save message incrementally", error)
    }
  }

  // Helper function to generate a step summary using the weak model (if dual-model enabled)
  const generateStepSummary = async (
    stepNumber: number,
    toolCalls?: MCPToolCall[],
    toolResults?: MCPToolResult[],
    assistantResponse?: string,
    isCompletion?: boolean,
  ) => {
    if (!isSummarizationEnabled()) {
      return null
    }

    const hasToolCalls = !!toolCalls && toolCalls.length > 0
    const isCompletionStep = isCompletion ?? false

    if (!shouldSummarizeStep(hasToolCalls, isCompletionStep)) {
      return null
    }

    const input: SummarizationInput = {
      sessionId: currentSessionId,
      stepNumber,
      toolCalls: toolCalls?.map(tc => ({
        name: tc.name,
        arguments: tc.arguments,
      })),
      toolResults: toolResults?.map(tr => ({
        success: !tr.isError,
        content: Array.isArray(tr.content)
          ? tr.content.map(c => c.text).join("\n")
          : String(tr.content || ""),
        error: tr.isError
          ? (Array.isArray(tr.content) ? tr.content.map(c => c.text).join("\n") : String(tr.content || ""))
          : undefined,
      })),
      assistantResponse,
      recentMessages: conversationHistory.slice(-5).map(m => ({
        role: m.role,
        content: m.content,
      })),
    }

    try {
      const summary = await summarizeAgentStep(input)
      if (summary) {
        summarizationService.addSummary(summary)

        // Auto-save all summaries if enabled (no importance threshold)
        // Associate memory with the session's profile for profile-scoped memories
        if (config.memoriesEnabled !== false && config.dualModelAutoSaveImportant) {
          const profileIdForMemory = effectiveProfileSnapshot?.profileId ?? config.mcpCurrentProfileId
          const memory = memoryService.createMemoryFromSummary(
            summary,
            undefined, // title
            undefined, // userNotes
            undefined, // tags
            undefined, // conversationTitle
            currentConversationId,
            profileIdForMemory,
          )
          memoryService.saveMemory(memory).catch(err => {
            if (isDebugLLM()) {
              logLLM("[Dual-Model] Error auto-saving summary:", err)
            }
          })
        }

        if (isDebugLLM()) {
          logLLM("[Dual-Model] Generated step summary:", {
            stepNumber: summary.stepNumber,
            importance: summary.importance,
            actionSummary: summary.actionSummary,
          })
        }

        return summary
      }
    } catch (error) {
      if (isDebugLLM()) {
        logLLM("[Dual-Model] Error generating step summary:", error)
      }
    }

    return null
  }

  // Helper function to add a message to conversation history AND save it incrementally
  // This ensures all messages are both in memory and persisted to disk
  const addMessage = (
    role: "user" | "assistant" | "tool",
    content: string,
    toolCalls?: MCPToolCall[],
    toolResults?: MCPToolResult[],
    timestamp?: number
  ) => {
    // Add to in-memory history
    const message: typeof conversationHistory[0] = {
      role,
      content,
      toolCalls,
      toolResults,
      timestamp: timestamp || Date.now()
    }
    conversationHistory.push(message)

    // Save to disk asynchronously (fire and forget)
    saveMessageIncremental(role, content, toolCalls, toolResults).catch(err => {
      logLLM("[addMessage] Failed to save message:", err)
    })
  }

  // Track current iteration for retry progress callback
  // This is updated in the agent loop and read by onRetryProgress
  let currentIterationRef = 0

  // Create retry progress callback that emits updates to the UI
  // This callback is passed to makeLLMCall to show retry status
  // Note: This callback captures conversationHistory and formatConversationForProgress by reference,
  // so it will have access to them when called (they are defined later in this function)
  const onRetryProgress: RetryProgressCallback = (retryInfo) => {
    emit({
      currentIteration: currentIterationRef,
      maxIterations,
      steps: [], // Empty - retry info is separate from steps
      isComplete: false,
      retryInfo: retryInfo.isRetrying ? retryInfo : undefined,
      // Include conversationHistory to avoid "length: 0" logs in emitAgentProgress
      conversationHistory: typeof formatConversationForProgress === 'function' && conversationHistory
        ? formatConversationForProgress(conversationHistory)
        : [],
    })
  }

  // Initialize progress tracking
  const progressSteps: AgentProgressStep[] = []

  // Add initial step
  const initialStep = createProgressStep(
    "thinking",
    "Analyzing request",
    "Processing your request and determining next steps",
    "in_progress",
  )
  progressSteps.push(initialStep)

  // Update initial step with tool count
  initialStep.status = "completed"
  initialStep.description = `Found ${availableTools.length} available tools.`

  // Remove duplicates from available tools to prevent confusion
  const uniqueAvailableTools = availableTools.filter(
    (tool, index, self) =>
      index === self.findIndex((t) => t.name === tool.name),
  )

  // Use profile snapshot for session isolation if available, otherwise fall back to global config
  // This ensures the session uses the profile settings at creation time,
  // even if the global profile is changed during session execution
  const agentModeGuidelines = effectiveProfileSnapshot?.guidelines ?? config.mcpToolsSystemPrompt ?? ""
  const customSystemPrompt = effectiveProfileSnapshot?.systemPrompt ?? config.mcpCustomSystemPrompt
  // Get skills instructions from profile snapshot (typically set by personas)
  const personaSkillsInstructions = effectiveProfileSnapshot?.skillsInstructions
  // Get persona properties from profile snapshot (dynamic key-value pairs)
  const personaProperties = effectiveProfileSnapshot?.personaProperties

  // Load enabled agent skills instructions for the current profile
  // Skills provide specialized instructions that improve AI performance on specific tasks
  // Use per-profile skills config if available, otherwise fall back to empty (no skills)
  const { skillsService } = await import("./skills-service")
  const enabledSkillIds = effectiveProfileSnapshot?.skillsConfig?.enabledSkillIds ?? []
  logLLM(`[processTranscriptWithAgentMode] Loading skills for session ${currentSessionId}. enabledSkillIds: [${enabledSkillIds.join(', ')}]`)
  const profileSkillsInstructions = skillsService.getEnabledSkillsInstructionsForProfile(enabledSkillIds)
  logLLM(`[processTranscriptWithAgentMode] Skills instructions loaded: ${profileSkillsInstructions ? `${profileSkillsInstructions.length} chars` : 'none'}`)

  // Combine persona-level and profile-level skills instructions
  const skillsInstructions = [personaSkillsInstructions, profileSkillsInstructions].filter(Boolean).join('\n\n') || undefined

  // Load memories for agent context (works independently of dual-model summarization)
  // Memories provide context from previous sessions - user preferences, past decisions, important learnings
  // Memories are filtered by the session's profile
  // Only load if both memoriesEnabled (system-wide) and dualModelInjectMemories are true
  let relevantMemories: AgentMemory[] = []
  if (config.memoriesEnabled !== false && config.dualModelInjectMemories) {
    const profileIdForMemories = effectiveProfileSnapshot?.profileId ?? config.mcpCurrentProfileId
    const allMemories = profileIdForMemories
      ? await memoryService.getMemoriesByProfile(profileIdForMemories)
      : await memoryService.getAllMemories()
    // Sort by importance first (critical > high > medium > low), then by recency, before capping
    const importanceOrder: Record<string, number> = { critical: 0, high: 1, medium: 2, low: 3 }
    const sortedMemories = [...allMemories].sort((a, b) => {
      const impDiff = importanceOrder[a.importance] - importanceOrder[b.importance]
      if (impDiff !== 0) return impDiff
      return b.createdAt - a.createdAt // More recent first as tiebreaker
    })
    relevantMemories = sortedMemories.slice(0, 30) // Cap at 30 for agent mode
    logLLM(`[processTranscriptWithAgentMode] Loaded ${relevantMemories.length} memories for context (from ${allMemories.length} total, profile: ${profileIdForMemories || 'global'})`)
  }

  // Construct system prompt using the new approach
  const systemPrompt = constructSystemPrompt(
    uniqueAvailableTools,
    agentModeGuidelines,
    true,
    undefined, // relevantTools removed - let LLM decide tool relevance
    customSystemPrompt, // custom base system prompt from profile snapshot or global config
    skillsInstructions, // agent skills instructions
    personaProperties, // dynamic persona properties
    relevantMemories, // memories from previous sessions
  )

  logLLM(`[llm.ts processTranscriptWithAgentMode] Initializing conversationHistory for session ${currentSessionId}`)
  logLLM(`[llm.ts processTranscriptWithAgentMode] previousConversationHistory length: ${previousConversationHistory?.length || 0}`)
  if (previousConversationHistory && previousConversationHistory.length > 0) {
    logLLM(`[llm.ts processTranscriptWithAgentMode] previousConversationHistory roles: [${previousConversationHistory.map(m => m.role).join(', ')}]`)
  }

  const conversationHistory: Array<{
    role: "user" | "assistant" | "tool"
    content: string
    toolCalls?: MCPToolCall[]
    toolResults?: MCPToolResult[]
    timestamp?: number
  }> = [
    ...(previousConversationHistory || []),
    { role: "user", content: transcript, timestamp: Date.now() },
  ]

  // Track the index where the current user prompt was added
  // This is used to scope tool result checks to only the current turn
  const currentPromptIndex = previousConversationHistory?.length || 0

  logLLM(`[llm.ts processTranscriptWithAgentMode] conversationHistory initialized with ${conversationHistory.length} messages, roles: [${conversationHistory.map(m => m.role).join(', ')}]`)

  // Save the initial user message incrementally
  // Only save if this is a new message (not already in previous conversation history)
  // Check if ANY user message in previousConversationHistory has the same content (not just the last one)
  // This handles retry scenarios where the user message exists but isn't the last message
  // (e.g., after a failed attempt that added assistant/tool messages)
  const userMessageAlreadyExists = previousConversationHistory?.some(
    msg => msg.role === "user" && msg.content === transcript
  ) ?? false
  if (!userMessageAlreadyExists) {
    saveMessageIncremental("user", transcript).catch(err => {
      logLLM("[processTranscriptWithAgentMode] Failed to save initial user message:", err)
    })
  }

  // Track empty response retries to prevent infinite loops
  let emptyResponseRetryCount = 0

  // Helper function to convert conversation history to the format expected by AgentProgressUpdate
  const formatConversationForProgress = (
    history: typeof conversationHistory,
  ) => {
    const isNudge = (content: string) =>
      content.includes("Please either take action using available tools") ||
      content.includes("You have relevant tools available for this request") ||
      content.includes("Your previous response was empty") ||
      content.includes("Verifier indicates the task is not complete") ||
      content.includes("Please respond with a valid JSON object")

    return history
      .filter((entry) => !(entry.role === "user" && isNudge(entry.content)))
      .map((entry) => ({
        role: entry.role,
        content: entry.content,
        toolCalls: entry.toolCalls?.map((tc) => ({
          name: tc.name,
          arguments: tc.arguments,
        })),
        toolResults: entry.toolResults?.map((tr) => {
          // Safely handle content - it should be an array, but add defensive check
          const contentText = Array.isArray(tr.content)
            ? tr.content.map((c) => c.text).join("\n")
            : String(tr.content || "")

          return {
            success: !tr.isError,
            content: contentText,
            error: tr.isError ? contentText : undefined,
          }
        }),
        // Preserve original timestamp if available, otherwise use current time
        timestamp: entry.timestamp || Date.now(),
      }))
  }

  // Helper to check if content is just a tool call placeholder (not real content)
  const isToolCallPlaceholder = (content: string): boolean => {
    const trimmed = content.trim()
    // Match patterns like "[Calling tools: ...]" or "[Tool: ...]"
    return /^\[(?:Calling tools?|Tool|Tools?):[^\]]+\]$/i.test(trimmed)
  }

  // Helper to detect if agent is repeating the same response (infinite loop)
  const detectRepeatedResponse = (currentResponse: string): boolean => {
    // Get last 3 assistant responses (excluding the current one)
    const assistantResponses = conversationHistory
      .filter(entry => entry.role === "assistant")
      .map(entry => entry.content.trim().toLowerCase())
      .slice(-3)

    // Allow detection with at least 1 previous response (detect loops earlier)
    if (assistantResponses.length < 1) return false

    const currentTrimmed = currentResponse.trim().toLowerCase()
    if (currentTrimmed.length === 0) return false

    // Check if current response is very similar to any of the last 2 responses
    for (const prevResponse of assistantResponses.slice(-2)) {
      if (prevResponse.length === 0) continue

      // For very short responses (< 5 words), require exact match to avoid false positives
      // Single-word responses like "yes", "done", "ok" could be legitimately repeated
      const wordCount = currentTrimmed.split(/\s+/).length
      if (wordCount < 5) {
        if (currentTrimmed === prevResponse) {
          return true
        }
        continue
      }

      // For longer responses, use Jaccard similarity with 80% threshold
      const similarity = calculateSimilarity(currentTrimmed, prevResponse)
      if (similarity > 0.8) {
        return true
      }
    }

    return false
  }

  // Simple similarity calculation (Jaccard similarity on words)
  const calculateSimilarity = (str1: string, str2: string): number => {
    const words1 = new Set(str1.split(/\s+/))
    const words2 = new Set(str2.split(/\s+/))

    const intersection = new Set([...words1].filter(x => words2.has(x)))
    const union = new Set([...words1, ...words2])

    return union.size === 0 ? 0 : intersection.size / union.size
  }

  // Helper to map conversation history to LLM messages format (filters empty content)
  const mapConversationToMessages = (
    addSummaryPrompt: boolean = false
  ): Array<{ role: "user" | "assistant"; content: string }> => {
    const mapped = conversationHistory
      .map((entry) => {
        if (entry.role === "tool") {
          const text = (entry.content || "").trim()
          if (!text) return null
          // Tool results already contain tool name prefix (format: [toolName] content...)
          // Just pass through without adding generic "Tool execution results:" wrapper
          return { role: "user" as const, content: text }
        }
        const content = (entry.content || "").trim()
        if (!content) return null
        return { role: entry.role as "user" | "assistant", content }
      })
      .filter(Boolean) as Array<{ role: "user" | "assistant"; content: string }>

    // Add summary prompt if last message is from assistant (ensures LLM has something to respond to)
    if (addSummaryPrompt && mapped.length > 0 && mapped[mapped.length - 1].role === "assistant") {
      mapped.push({ role: "user", content: "Please provide a brief summary of what was accomplished." })
    }
    return mapped
  }

  // Helper to generate post-verify summary (consolidates duplicate logic)
  const generatePostVerifySummary = async (
    currentFinalContent: string,
    checkForStop: boolean = false,
    activeToolsList: MCPTool[] = uniqueAvailableTools
  ): Promise<{ content: string; stopped: boolean }> => {
    const postVerifySummaryStep = createProgressStep(
      "thinking",
      "Summarizing results",
      "Creating a concise final summary of what was achieved",
      "in_progress",
    )
    progressSteps.push(postVerifySummaryStep)
    emit({
      currentIteration: iteration,
      maxIterations,
      steps: progressSteps.slice(-3),
      isComplete: false,
      conversationHistory: formatConversationForProgress(conversationHistory),
    })

    const postVerifySystemPrompt = constructSystemPrompt(
      activeToolsList,
      agentModeGuidelines, // Use session-bound guidelines
      true,
      undefined, // relevantTools removed
      customSystemPrompt, // Use session-bound custom system prompt
      skillsInstructions, // agent skills instructions
      personaProperties, // dynamic persona properties
      relevantMemories, // memories from previous sessions
    )

    const postVerifySummaryMessages = [
      { role: "system" as const, content: postVerifySystemPrompt },
      ...mapConversationToMessages(true),
    ]

    const { messages: shrunkMessages, estTokensAfter: verifyEstTokens, maxTokens: verifyMaxTokens } = await shrinkMessagesForLLM({
      messages: postVerifySummaryMessages as any,
      availableTools: activeToolsList,
      relevantTools: undefined,
      isAgentMode: true,
      sessionId: currentSessionId,
      onSummarizationProgress: (current, total) => {
        const lastThinkingStep = progressSteps.findLast(step => step.type === "thinking")
        if (lastThinkingStep) {
          lastThinkingStep.description = `Summarizing for verification (${current}/${total})`
        }
        emit({
          currentIteration: iteration,
          maxIterations,
          steps: progressSteps.slice(-3),
          isComplete: false,
          conversationHistory: formatConversationForProgress(conversationHistory),
        })
      },
    })
    // Update context info for progress display
    contextInfoRef = { estTokens: verifyEstTokens, maxTokens: verifyMaxTokens }

    const response = await makeLLMCall(shrunkMessages, config, onRetryProgress, undefined, currentSessionId)

    // Check for stop request if needed
    if (checkForStop && agentSessionStateManager.shouldStopSession(currentSessionId)) {
      logLLM(`Agent session ${currentSessionId} stopped during post-verify summary generation`)
      return { content: currentFinalContent, stopped: true }
    }

    postVerifySummaryStep.status = "completed"
    postVerifySummaryStep.llmContent = response.content || ""
    postVerifySummaryStep.title = "Summary provided"
    postVerifySummaryStep.description = response.content && response.content.length > 100
      ? response.content.substring(0, 100) + "..."
      : response.content || "Summary generated"

    return { content: response.content || currentFinalContent, stopped: false }
  }

  // Build compact verification messages (schema-first verifier)
  const buildVerificationMessages = (finalAssistantText: string, currentVerificationFailCount: number = 0) => {
    const maxItems = Math.max(1, config.mcpVerifyContextMaxItems || 20)
    const recent = conversationHistory.slice(-maxItems)
    const messages: Array<{ role: "user" | "assistant" | "system"; content: string }> = []

    // Track the last assistant content added to avoid duplicates
    let lastAddedAssistantContent: string | null = null

    messages.push({
      role: "system",
      content:
        `You are a completion verifier. Determine if the user's original request has been FULLY DELIVERED to the user.

FIRST, CHECK THESE BLOCKERS (if ANY are true, mark INCOMPLETE):
- The agent stated intent to do more work (e.g., "Let me...", "I'll...", "Now I'll...", "I'm going to...")
- The agent's response is a status update rather than a deliverable (e.g., "I've extracted the data" without presenting results)
- The user asked for information/analysis that was NOT directly provided in the agent's response
- Tool results exist but the agent hasn't synthesized/presented them to the user
- The response is empty or just acknowledges the request

ONLY IF NO BLOCKERS, mark COMPLETE if:
1. The agent directly answered the user's question or fulfilled their request
2. The agent explained why the request is impossible and cannot proceed
3. The agent is asking for clarification needed to proceed
4. The agent explicitly confirmed completion ("Done", "Here's your summary", "Task complete")

IMPORTANT - Do NOT mark complete just because:
- Tools executed successfully (results must be PRESENTED to user)
- Data was gathered (it must be SUMMARIZED/DELIVERED)
- The agent made progress (the FINAL deliverable must exist)

Return ONLY JSON per schema.`,
    })
    messages.push({ role: "user", content: `Original request:\n${transcript}` })
    for (const entry of recent) {
      if (entry.role === "tool") {
        const text = (entry.content || "").trim()
        // Tool results already contain tool name prefix (format: [toolName] content...)
        // Pass through directly without adding redundant wrapper
        messages.push({ role: "user", content: text || "[No tool output]" })
      } else if (entry.role === "user") {
        // Skip empty user messages
        const text = (entry.content || "").trim()
        if (text) {
          messages.push({ role: "user", content: text })
        }
      } else {
        // Ensure non-empty content for assistant messages (Anthropic API requirement)
        let content = entry.content
        if (entry.role === "assistant" && !content?.trim()) {
          if (entry.toolCalls && entry.toolCalls.length > 0) {
            const toolNames = entry.toolCalls.map(tc => tc.name).join(", ")
            content = `[Calling tools: ${toolNames}]`
          } else {
            content = "[Processing...]"
          }
        }
        messages.push({ role: entry.role, content })
        if (entry.role === "assistant") {
          lastAddedAssistantContent = content
        }
      }
    }
    // Only add finalAssistantText if it's different from the last assistant message added
    if (finalAssistantText?.trim() && finalAssistantText.trim() !== lastAddedAssistantContent?.trim()) {
      messages.push({ role: "assistant", content: finalAssistantText })
    }

    // Build the JSON request with optional verification attempt note (combined into single message)
    let jsonRequestContent = "Return a JSON object with fields: isComplete (boolean), confidence (0..1), missingItems (string[]), reason (string). No extra commentary."
    if (currentVerificationFailCount > 0) {
      jsonRequestContent += `\n\nNote: This is verification attempt #${currentVerificationFailCount + 1}. If the task appears reasonably complete, please mark as complete to avoid infinite loops.`
    }
    messages.push({ role: "user", content: jsonRequestContent })

    return messages
  }

  // Verification failure limit - after this many failures, force completion
  const VERIFICATION_FAIL_LIMIT = 5

  // Empty response retry limit - after this many retries, break to prevent infinite loops
  const MAX_EMPTY_RESPONSE_RETRIES = 3

  /**
   * Result of running verification and handling the outcome
   */
  interface VerificationHandlerResult {
    /** Whether the loop should continue (verification failed and we should retry) */
    shouldContinue: boolean
    /** Whether verification passed or we hit the limit (task is done either way) */
    isComplete: boolean
    /** Updated verification failure count */
    newFailCount: number
    /** Whether to skip post-verify summary (e.g., when repeating) */
    skipPostVerifySummary: boolean
  }

  /**
   * Centralized verification handler - eliminates duplicated verification logic.
   *
   * This function:
   * 1. Checks for repeated responses (infinite loop detection)
   * 2. Calls the verifier with retries
   * 3. Handles verification failure (nudges, hard limits)
   * 4. Updates the verifyStep status
   *
   * @param finalContent - The content to verify
   * @param verifyStep - The progress step to update
   * @param currentFailCount - Current verification failure count
   * @param options - Additional options for specific verification scenarios
   */
  async function runVerificationAndHandleResult(
    finalContent: string,
    verifyStep: AgentProgressStep,
    currentFailCount: number,
    options: {
      /** Custom nudge message for when verification fails */
      customNudgePrefix?: string
      /** Whether to check for repeated responses */
      checkRepeating?: boolean
      /** Whether to add tool usage nudge after 2 failures */
      nudgeForToolUsage?: boolean
      /** Index where the current user prompt was added (for scoping tool result checks) */
      currentPromptIndex?: number
    } = {}
  ): Promise<VerificationHandlerResult> {
    const {
      customNudgePrefix = "Verifier indicates the task is not complete.",
      checkRepeating = true,
      nudgeForToolUsage = false,
      currentPromptIndex: promptIndex,
    } = options

    const retries = Math.max(0, config.mcpVerifyRetryCount ?? 1)
    let verified = false
    let verification: any = null
    let skipPostVerifySummary = false

    // Check for infinite loop (repeated responses)
    if (checkRepeating) {
      const isRepeating = detectRepeatedResponse(finalContent)
      if (isRepeating) {
        verified = true
        // Only skip post-verify summary if we have real content (not just a tool call placeholder)
        if (!isToolCallPlaceholder(finalContent) && finalContent.trim().length > 0) {
          skipPostVerifySummary = true
        }
        verifyStep.status = "completed"
        verifyStep.description = "Agent response is repeating - accepting as final"
        if (isDebugLLM()) {
          logLLM("Infinite loop detected - treating as complete", {
            finalContent: finalContent.substring(0, 200),
            isPlaceholder: isToolCallPlaceholder(finalContent),
            willGenerateSummary: !skipPostVerifySummary
          })
        }
        return {
          shouldContinue: false,
          isComplete: true,
          newFailCount: 0,
          skipPostVerifySummary
        }
      }
    }

    // Run verification with retries
    for (let i = 0; i <= retries; i++) {
      verification = await verifyCompletionWithFetch(
        buildVerificationMessages(finalContent, currentFailCount),
        config.mcpToolsProviderId,
        currentSessionId // Pass session ID for Langfuse tracing and abort signal handling
      )
      if (verification?.isComplete === true) {
        verified = true
        break
      }
    }

    // Verification passed
    if (verified) {
      verifyStep.status = "completed"
      verifyStep.description = "Verification passed"
      return {
        shouldContinue: false,
        isComplete: true,
        newFailCount: 0, // Reset on success
        skipPostVerifySummary
      }
    }

    // Verification failed - handle it
    const newFailCount = currentFailCount + 1

    // Hard limit on verification failures to prevent infinite loops
    // Check this BEFORE pushing the user nudge to avoid "please continue" message when force-completing
    if (newFailCount >= VERIFICATION_FAIL_LIMIT) {
      logLLM(`âš ï¸ Verification failed ${VERIFICATION_FAIL_LIMIT} times - forcing completion`)
      verifyStep.status = "completed"
      verifyStep.description = "Verification limit reached - accepting as complete"
      return {
        shouldContinue: false,
        isComplete: true, // Force complete
        newFailCount,
        skipPostVerifySummary
      }
    }

    // Only push the nudge message if we're going to continue (not at hard limit)
    verifyStep.status = "error"
    verifyStep.description = "Verification failed: continuing to address missing items"

    const missing = (verification?.missingItems || [])
      .filter((s: string) => s && s.trim())
      .map((s: string) => `- ${s}`)
      .join("\n")
    const reason = verification?.reason ? `Reason: ${verification.reason}` : ""
    const userNudge = `${customNudgePrefix}\n${reason}\n${missing ? `Missing items:\n${missing}` : ""}\nPlease continue and complete the remaining work.`
    conversationHistory.push({ role: "user", content: userNudge, timestamp: Date.now() })

    // Optional: nudge for tool usage if no tools have been executed
    if (nudgeForToolUsage && newFailCount >= 2) {
      // Scope to current turn if promptIndex is provided, otherwise check entire conversation
      const hasToolResultsSoFar = promptIndex !== undefined
        ? toolsExecutedInSession || conversationHistory.slice(promptIndex + 1).some((e) => e.role === "tool")
        : toolsExecutedInSession || conversationHistory.some((e) => e.role === "tool")
      if (!hasToolResultsSoFar) {
        conversationHistory.push({
          role: "user",
          content: "Important: Do not just state intent. Use the available tools by calling them directly via the native function calling interface to complete the task.",
          timestamp: Date.now()
        })
      }
    }

    return {
      shouldContinue: true,
      isComplete: false,
      newFailCount,
      skipPostVerifySummary
    }
  }

  // Emit initial progress
  emit({
    currentIteration: 0,
    maxIterations,
    steps: progressSteps.slice(-3), // Show max 3 steps
    isComplete: false,
    conversationHistory: formatConversationForProgress(conversationHistory),
  })

  let noOpCount = 0 // Track iterations without meaningful progress
  let totalNudgeCount = 0 // Track total nudges to prevent infinite nudge loops
  const MAX_NUDGES = 3 // Max nudges before accepting text response as complete
  let verificationFailCount = 0 // Count consecutive verification failures to avoid loops
  const toolFailureCount = new Map<string, number>() // Track failures per tool name
  const MAX_TOOL_FAILURES = 3 // Max times a tool can fail before being excluded

  while (iteration < maxIterations) {
    iteration++
    currentIterationRef = iteration // Update ref for retry progress callback

    // Filter out tools that have failed too many times - compute at start of iteration
    // so the same filtered list is used consistently throughout (LLM call + heuristics)
    const activeTools = uniqueAvailableTools.filter(tool => {
      const failures = toolFailureCount.get(tool.name) || 0
      return failures < MAX_TOOL_FAILURES
    })

    // Log when tools have been excluded
    const excludedToolCount = uniqueAvailableTools.length - activeTools.length
    if (excludedToolCount > 0 && iteration === 1) {
      // Only log on first iteration after exclusion to avoid spam
      logLLM(`â„¹ï¸ ${excludedToolCount} tool(s) excluded due to repeated failures`)
    }

    // Rebuild system prompt if tools were excluded to keep LLM's view of tools in sync
    // This ensures the system prompt lists only the tools that are actually available
    let currentSystemPrompt = systemPrompt
    if (excludedToolCount > 0) {
      currentSystemPrompt = constructSystemPrompt(
        activeTools,
        agentModeGuidelines,
        true,
        undefined, // relevantTools removed - let LLM decide tool relevance
        customSystemPrompt, // custom base system prompt from profile snapshot or global config
        skillsInstructions, // agent skills instructions
      )
      logLLM(`[processTranscriptWithAgentMode] Rebuilt system prompt with ${activeTools.length} active tools (excluded ${excludedToolCount})`)
    }

    // Check for stop signal (session-specific or global)
    if (agentSessionStateManager.shouldStopSession(currentSessionId)) {
      logLLM(`Agent session ${currentSessionId} stopped by kill switch`)

      // Add emergency stop step
      const stopStep = createProgressStep(
        "completion",
        "Agent stopped",
        "Agent mode was stopped by emergency kill switch",
        "error",
      )
      progressSteps.push(stopStep)

      // Emit final progress (ensure final output is saved in history)
      const killNote = "\n\n(Agent mode was stopped by emergency kill switch)"
      const finalOutput = (finalContent || "") + killNote
      conversationHistory.push({ role: "assistant", content: finalOutput, timestamp: Date.now() })
      emit({
        currentIteration: iteration,
        maxIterations,
        steps: progressSteps.slice(-3),
        isComplete: true,
        finalContent: finalOutput,
        conversationHistory: formatConversationForProgress(conversationHistory),
      })

      wasAborted = true
      break
    }

    // Update iteration count in session state
    agentSessionStateManager.updateIterationCount(currentSessionId, iteration)

    // Update initial step to completed and add thinking step for this iteration
    if (iteration === 1) {
      initialStep.status = "completed"
    }

    const thinkingStep = createProgressStep(
      "thinking",
      `Processing request (iteration ${iteration})`,
      "Analyzing request and planning next actions",
      "in_progress",
    )
    progressSteps.push(thinkingStep)

    // Emit progress update for thinking step
    emit({
      currentIteration: iteration,
      maxIterations,
      steps: progressSteps.slice(-3),
      isComplete: false,
      conversationHistory: formatConversationForProgress(conversationHistory),
    })

    // Build messages for LLM call
    const messages = [
      { role: "system", content: currentSystemPrompt },
      ...conversationHistory
        .map((entry) => {
          if (entry.role === "tool") {
            const text = (entry.content || "").trim()
            if (!text) return null
            // Tool results already contain tool name prefix (format: [toolName] content...)
            // Pass through directly without adding redundant wrapper
            return {
              role: "user" as const,
              content: text,
            }
          }
          // For assistant messages, ensure non-empty content
          // Anthropic API requires all messages to have non-empty content
          // except for the optional final assistant message
          let content = entry.content
          if (entry.role === "assistant" && !content?.trim()) {
            // If assistant message has tool calls but no content, describe the tool calls
            if (entry.toolCalls && entry.toolCalls.length > 0) {
              const toolNames = entry.toolCalls.map(tc => tc.name).join(", ")
              content = `[Calling tools: ${toolNames}]`
            } else {
              // Fallback for empty assistant messages without tool calls
              content = "[Processing...]"
            }
          }
          return {
            role: entry.role as "user" | "assistant",
            content,
          }
        })
        .filter(Boolean as any),
    ]

    // Apply context budget management before the agent LLM call
    // All active tools are sent to the LLM - progressive disclosure tools
    // (list_server_tools, get_tool_schema) allow the LLM to discover tools dynamically
    const { messages: shrunkMessages, estTokensAfter, maxTokens: maxContextTokens } = await shrinkMessagesForLLM({
      messages: messages as any,
      availableTools: activeTools,
      relevantTools: undefined,
      isAgentMode: true,
      sessionId: currentSessionId,
      onSummarizationProgress: (current, total, message) => {
        // Update thinking step with summarization progress
        thinkingStep.description = `Summarizing context (${current}/${total})`
        thinkingStep.llmContent = message
        emit({
          currentIteration: iteration,
          maxIterations,
          steps: progressSteps.slice(-3),
          isComplete: false,
          conversationHistory: formatConversationForProgress(conversationHistory),
        })
      },
    })
    // Update context info for progress display
    contextInfoRef = { estTokens: estTokensAfter, maxTokens: maxContextTokens }

    // If stop was requested during context shrinking, exit now
    if (agentSessionStateManager.shouldStopSession(currentSessionId)) {
      logLLM(`Agent session ${currentSessionId} stopped during context shrink`)
      thinkingStep.status = "completed"
      thinkingStep.title = "Agent stopped"
      thinkingStep.description = "Emergency stop triggered"
      const killNote = "\n\n(Agent mode was stopped by emergency kill switch)"
      const finalOutput = (finalContent || "") + killNote
      conversationHistory.push({ role: "assistant", content: finalOutput, timestamp: Date.now() })
      emit({
        currentIteration: iteration,
        maxIterations,
        steps: progressSteps.slice(-3),
        isComplete: true,
        finalContent: finalOutput,
        conversationHistory: formatConversationForProgress(conversationHistory),
      })
      wasAborted = true
      break
    }

    // Make LLM call (abort-aware) with streaming for real-time UI updates
    let llmResponse: any
    try {
      // Create streaming callback that emits progress updates as content streams in
      let lastStreamEmitTime = 0
      const STREAM_EMIT_THROTTLE_MS = 50

      const onStreamingUpdate: StreamingCallback = (_chunk, accumulated) => {
        const now = Date.now()
        // Update the thinking step with streaming content (always)
        thinkingStep.llmContent = accumulated

        // Throttle emit calls to reduce log spam
        if (now - lastStreamEmitTime < STREAM_EMIT_THROTTLE_MS) {
          return // Skip emit, but content is updated
        }
        lastStreamEmitTime = now

        // Emit progress update with streaming content
        emit({
          currentIteration: iteration,
          maxIterations,
          steps: progressSteps.slice(-3),
          isComplete: false,
          conversationHistory: formatConversationForProgress(conversationHistory),
          streamingContent: {
            text: accumulated,
            isStreaming: true,
          },
        })
      }

      llmResponse = await makeLLMCall(shrunkMessages, config, onRetryProgress, onStreamingUpdate, currentSessionId, activeTools)

      // Clear streaming state after response is complete
      emit({
        currentIteration: iteration,
        maxIterations,
        steps: progressSteps.slice(-3),
        isComplete: false,
        conversationHistory: formatConversationForProgress(conversationHistory),
        streamingContent: {
          text: llmResponse?.content || "",
          isStreaming: false,
        },
      })

      // If stop was requested while the LLM call was in-flight and it returned before aborting, exit now
      if (agentSessionStateManager.shouldStopSession(currentSessionId)) {
        logLLM(`Agent session ${currentSessionId} stopped right after LLM response`)
        thinkingStep.status = "completed"
        thinkingStep.title = "Agent stopped"
        thinkingStep.description = "Emergency stop triggered"
        const killNote = "\n\n(Agent mode was stopped by emergency kill switch)"
        const finalOutput = (finalContent || "") + killNote
        conversationHistory.push({ role: "assistant", content: finalOutput, timestamp: Date.now() })
        emit({
          currentIteration: iteration,
          maxIterations,
          steps: progressSteps.slice(-3),
          isComplete: true,
          finalContent: finalOutput,
          conversationHistory: formatConversationForProgress(conversationHistory),
        })
        wasAborted = true
        break
      }
    } catch (error: any) {
      if (error?.name === "AbortError" || agentSessionStateManager.shouldStopSession(currentSessionId)) {
        logLLM(`LLM call aborted for session ${currentSessionId} due to emergency stop`)
        thinkingStep.status = "completed"
        thinkingStep.title = "Agent stopped"
        thinkingStep.description = "Emergency stop triggered"
        // Ensure final output appears in saved conversation on abort
        const killNote = "\n\n(Agent mode was stopped by emergency kill switch)"
        const finalOutput = (finalContent || "") + killNote
        conversationHistory.push({ role: "assistant", content: finalOutput, timestamp: Date.now() })
        emit({
          currentIteration: iteration,
          maxIterations,
          steps: progressSteps.slice(-3),
          isComplete: true,
          finalContent: finalOutput,
          conversationHistory: formatConversationForProgress(conversationHistory),
        })
        wasAborted = true
        break
      }

      // Handle empty response errors - retry with guidance
      const errorMessage = (error?.message || String(error)).toLowerCase()
      if (errorMessage.includes("empty") || errorMessage.includes("no text") || errorMessage.includes("no content")) {
        emptyResponseRetryCount++
        if (emptyResponseRetryCount >= MAX_EMPTY_RESPONSE_RETRIES) {
          logLLM(`âŒ Empty response retry limit exceeded (${MAX_EMPTY_RESPONSE_RETRIES} retries)`)
          diagnosticsService.logError("llm", "Empty response retry limit exceeded", {
            iteration,
            retryCount: emptyResponseRetryCount,
            limit: MAX_EMPTY_RESPONSE_RETRIES
          })
          thinkingStep.status = "error"
          thinkingStep.description = "Empty response limit exceeded"
          const emptyResponseFinalContent = "I encountered repeated empty responses and couldn't complete the task. Please try again."
          conversationHistory.push({ role: "assistant", content: emptyResponseFinalContent, timestamp: Date.now() })
          emit({
            currentIteration: iteration,
            maxIterations,
            steps: progressSteps.slice(-3),
            isComplete: true,
            finalContent: emptyResponseFinalContent,
            conversationHistory: formatConversationForProgress(conversationHistory),
          })
          break
        }
        thinkingStep.status = "error"
        thinkingStep.description = "Empty response. Retrying..."
        emit({
          currentIteration: iteration,
          maxIterations,
          steps: progressSteps.slice(-3),
          isComplete: false,
          conversationHistory: formatConversationForProgress(conversationHistory),
        })
        addMessage("user", "Previous request had empty response. Please retry or summarize progress.")
        continue
      }

      // Other errors - throw (llm-fetch.ts handles JSON validation/failedGeneration recovery)
      throw error
    }

    // Validate response is not null/empty
    // A response is valid if it has either:
    // 1. Non-empty content, OR
    // 2. Valid toolCalls (tool-only responses have empty content), OR
    // 3. Empty content with needsMoreWork=false AND no toolCalls (LLM intentionally completed with finish_reason='stop')
    const hasValidContent = llmResponse?.content && llmResponse.content.trim().length > 0
    const hasValidToolCalls = llmResponse?.toolCalls && Array.isArray(llmResponse.toolCalls) && llmResponse.toolCalls.length > 0
    // Check for intentional empty completion (finish_reason='stop' in llm-fetch.ts returns this)
    // IMPORTANT: If there are toolCalls, they take precedence over intentional-empty completion
    // to ensure tool execution is not skipped
    const isIntentionalEmptyCompletion = llmResponse?.needsMoreWork === false && llmResponse?.content === "" && !hasValidToolCalls

    if (!llmResponse || (!hasValidContent && !hasValidToolCalls && !isIntentionalEmptyCompletion)) {
      emptyResponseRetryCount++
      logLLM(`âŒ LLM null/empty response on iteration ${iteration} (retry ${emptyResponseRetryCount}/${MAX_EMPTY_RESPONSE_RETRIES})`)
      logLLM("Response details:", {
        hasResponse: !!llmResponse,
        responseType: typeof llmResponse,
        responseKeys: llmResponse ? Object.keys(llmResponse) : [],
        content: llmResponse?.content,
        contentType: typeof llmResponse?.content,
        hasToolCalls: !!llmResponse?.toolCalls,
        toolCallsCount: llmResponse?.toolCalls?.length || 0,
        needsMoreWork: llmResponse?.needsMoreWork,
        fullResponse: JSON.stringify(llmResponse, null, 2)
      })
      diagnosticsService.logError("llm", "Null/empty LLM response in agent mode", {
        iteration,
        response: llmResponse,
        message: "LLM response has neither content nor toolCalls",
        retryCount: emptyResponseRetryCount,
        limit: MAX_EMPTY_RESPONSE_RETRIES
      })
      if (emptyResponseRetryCount >= MAX_EMPTY_RESPONSE_RETRIES) {
        logLLM(`âŒ Empty response retry limit exceeded (${MAX_EMPTY_RESPONSE_RETRIES} retries)`)
        thinkingStep.status = "error"
        thinkingStep.description = "Empty response limit exceeded"
        const emptyResponseFinalContent = "I encountered repeated empty responses and couldn't complete the task. Please try again."
        conversationHistory.push({ role: "assistant", content: emptyResponseFinalContent, timestamp: Date.now() })
        emit({
          currentIteration: iteration,
          maxIterations,
          steps: progressSteps.slice(-3),
          isComplete: true,
          finalContent: emptyResponseFinalContent,
          conversationHistory: formatConversationForProgress(conversationHistory),
        })
        break
      }
      thinkingStep.status = "error"
      thinkingStep.description = "Invalid response. Retrying..."
      emit({
        currentIteration: iteration,
        maxIterations,
        steps: progressSteps.slice(-3),
        isComplete: false,
        conversationHistory: formatConversationForProgress(conversationHistory),
      })
      // Check if recent messages contain truncated content that might be confusing
      const recentMessages = conversationHistory.slice(-3)
      const hasTruncatedContent = recentMessages.some(m =>
        m.content?.includes('[Truncated') ||
        m.content?.includes('[truncated]') ||
        m.content?.includes('(truncated')
      )
      const retryMessage = hasTruncatedContent
        ? "Previous request had empty response. The tool output was truncated which may have caused confusion. Please either: (1) try a different approach to get the data you need, (2) work with the partial data available, or (3) summarize your progress so far."
        : "Previous request had empty response. Please retry or summarize progress."
      addMessage("user", retryMessage)
      continue
    }

    // Reset empty response counter on successful response
    emptyResponseRetryCount = 0

    // Handle intentional empty completion from LLM (finish_reason='stop')
    // This is unusual - the model chose to complete without any content
    // We should verify this is actually complete before accepting it
    if (isIntentionalEmptyCompletion) {
      logLLM("âš ï¸ LLM intentionally completed with empty response (finish_reason=stop)")
      diagnosticsService.logWarning("llm", "LLM completed with empty response in agent mode", {
        iteration,
        needsMoreWork: llmResponse.needsMoreWork,
        message: "Model completed intentionally without content - will verify before accepting"
      })

      // Run verifier to check if task is actually complete before accepting empty completion
      if (config.mcpVerifyCompletionEnabled) {
        const verifyStep = createProgressStep(
          "thinking",
          "Verifying empty completion",
          "Checking if task was actually completed despite empty response",
          "in_progress",
        )
        progressSteps.push(verifyStep)
        emit({
          currentIteration: iteration,
          maxIterations,
          steps: progressSteps.slice(-3),
          isComplete: false,
          conversationHistory: formatConversationForProgress(conversationHistory),
        })

        // Build verification context from last assistant message with content
        const lastAssistantContent = conversationHistory
          .filter(m => m.role === "assistant" && m.content && m.content.trim().length > 0)
          .pop()?.content || ""

        const result = await runVerificationAndHandleResult(
          lastAssistantContent,
          verifyStep,
          verificationFailCount,
          {
            customNudgePrefix: "Your previous response was empty. The task is not complete.",
            checkRepeating: false, // Empty completions don't need repeat detection
          }
        )
        verificationFailCount = result.newFailCount

        if (result.shouldContinue) {
          logLLM("âš ï¸ Empty completion rejected by verifier - nudging LLM to continue")
          continue
        }

        logLLM("âœ… Empty completion verified - task is actually complete")
      }

      // Mark thinking step as completed
      thinkingStep.status = "completed"
      thinkingStep.title = "Agent completed"
      thinkingStep.description = "Model completed without additional content"

      // Add completion step
      const completionStep = createProgressStep(
        "completion",
        "Task completed",
        "The model completed without additional content",
        "completed",
      )
      progressSteps.push(completionStep)

      // Emit final progress with empty content
      emit({
        currentIteration: iteration,
        maxIterations,
        steps: progressSteps.slice(-3),
        isComplete: true,
        finalContent: "",
        conversationHistory: formatConversationForProgress(conversationHistory),
      })

      // End Langfuse trace for early completion
      if (isLangfuseEnabled()) {
        endAgentTrace(currentSessionId, {
          output: "",
          metadata: { totalIterations: iteration, earlyCompletion: true },
        })
        flushLangfuse().catch(() => {})
      }

      return {
        content: "",
        conversationHistory,
        totalIterations: iteration,
      }
    }

    // Update thinking step with actual LLM content and mark as completed
    thinkingStep.status = "completed"
    thinkingStep.llmContent = llmResponse.content || ""
    if (llmResponse.content) {
      // Update title and description to be more meaningful
      thinkingStep.title = "Agent response"
      thinkingStep.description =
        llmResponse.content.length > 100
          ? llmResponse.content.substring(0, 100) + "..."
          : llmResponse.content
    }

    // Emit progress update with the LLM content immediately after setting it
    emit({
      currentIteration: iteration,
      maxIterations,
      steps: progressSteps.slice(-3),
      isComplete: false,
      conversationHistory: formatConversationForProgress(conversationHistory),
    })

    // Check for explicit completion signal
    const toolCallsArray: MCPToolCall[] = Array.isArray(
      (llmResponse as any).toolCalls,
    )
      ? (llmResponse as any).toolCalls
      : []
    if (isDebugTools()) {
      if (
        (llmResponse as any).toolCalls &&
        !Array.isArray((llmResponse as any).toolCalls)
      ) {
        logTools("Non-array toolCalls received from LLM", {
          receivedType: typeof (llmResponse as any).toolCalls,
          value: (llmResponse as any).toolCalls,
        })
      }
      logTools("Planned tool calls from LLM", toolCallsArray)
    }
    const hasToolCalls = toolCallsArray.length > 0
    const explicitlyComplete = llmResponse.needsMoreWork === false

    if (explicitlyComplete && !hasToolCalls) {
      // Agent claims completion but provided no toolCalls.
      // If the content still contains tool-call markers, treat as not complete and nudge for structured toolCalls.
      const contentText = (llmResponse.content || "")
      const hasToolMarkers = /<\|tool_calls_section_begin\|>|<\|tool_call_begin\|>/i.test(contentText)
      if (hasToolMarkers) {
        conversationHistory.push({ role: "assistant", content: contentText.replace(/<\|[^|]*\|>/g, "").trim(), timestamp: Date.now() })
        conversationHistory.push({ role: "user", content: "Please use the native tool-calling interface to call the tools directly, rather than describing them in text.", timestamp: Date.now() })
        continue
      }

      // Check if there are actionable tools for this request
      // Use activeTools (filtered for failures) to avoid nudging for tools that are excluded
      const hasActionableTools = activeTools.length > 0
      // Scope to current turn using currentPromptIndex to avoid treating a new request
      // as having tools executed based on tool results from previous conversation turns
      const hasToolResultsSoFar = toolsExecutedInSession || conversationHistory.slice(currentPromptIndex + 1).some((e) => e.role === "tool")

      // Check if the response contains substantive content (a real answer, not a placeholder)
      // If the LLM explicitly sets needsMoreWork=false and provides a real answer,
      // we should trust it - even if there are tools that could theoretically be used.
      // This allows the agent to respond directly to simple questions without forcing tool calls.
      const hasSubstantiveContent = contentText.trim().length >= 1 && !isToolCallPlaceholder(contentText)

      // Only apply aggressive heuristics if:
      // 1. There are actually relevant tools for this request
      // 2. No tools have been used yet
      // 3. The agent's response doesn't contain substantive content (i.e., it's just a placeholder)
      if (hasActionableTools && !hasToolResultsSoFar && !hasSubstantiveContent) {
        // If there are actionable tools and no tool results yet, and no real answer provided,
        // nudge the model to produce structured toolCalls to actually perform the work.
        // Only add assistant message if non-empty to avoid blank entries
        if (contentText.trim().length > 0) {
          conversationHistory.push({ role: "assistant", content: contentText.trim(), timestamp: Date.now() })
        }
        conversationHistory.push({
          role: "user",
          content:
            "Before marking complete: please use the available tools to actually perform the steps. Call the tools directly using the native function calling interface.",
          timestamp: Date.now(),
        })
        noOpCount = 0
        continue
      }

      // Agent explicitly indicated completion and one of the following:
      // - No actionable tools exist for this request (simple Q&A), OR
      // - Tools were used and work is complete, OR
      // - Agent provided a substantive direct response (allows direct answers without tool calls)
      const assistantContent = llmResponse.content || ""

      finalContent = assistantContent
      // Note: Don't add message here - it will be added in the post-verify section
      // to avoid duplicate messages (the post-verify section handles all cases:
      // summary success, summary failure, and skip summary)

      // Optional verification before completing
      // Track if we should skip post-verify summary
      // Skip summary when:
      // 1. Final summary is disabled in config
      // 2. Agent is repeating itself (with real content)
      // 3. No tools were called (simple Q&A - nothing to summarize)
      const noToolsCalledYet = !conversationHistory.some((e) => e.role === "tool")
      let skipPostVerifySummary = (config.mcpFinalSummaryEnabled === false) || (noToolsCalledYet && !isToolCallPlaceholder(finalContent) && finalContent.trim().length > 0)

      if (config.mcpVerifyCompletionEnabled) {
        const verifyStep = createProgressStep(
          "thinking",
          "Verifying completion",
          "Checking that the user's request has been achieved",
          "in_progress",
        )
        progressSteps.push(verifyStep)
        emit({
          currentIteration: iteration,
          maxIterations,
          steps: progressSteps.slice(-3),
          isComplete: false,
          conversationHistory: formatConversationForProgress(conversationHistory),
        })

        const result = await runVerificationAndHandleResult(
          finalContent,
          verifyStep,
          verificationFailCount,
          {
            checkRepeating: true,
            nudgeForToolUsage: true, // This path needs tool usage nudges
            currentPromptIndex, // Scope tool result checks to current turn
          }
        )
        verificationFailCount = result.newFailCount
        if (result.skipPostVerifySummary) {
          skipPostVerifySummary = true
        }

        if (result.shouldContinue) {
          noOpCount = 0
          continue
        }
      }

        // Post-verify: produce a concise final summary for the user
        if (!skipPostVerifySummary) {
          try {
            const result = await generatePostVerifySummary(finalContent, false, activeTools)
            finalContent = result.content
            if (finalContent.trim().length > 0) {
              addMessage("assistant", finalContent)
            }
          } catch (e) {
            // If summary generation fails, still add the existing finalContent to history
            if (finalContent.trim().length > 0) {
              addMessage("assistant", finalContent)
            }
          }
        } else {
          // Even when skipping post-verify summary, ensure the final content is in history
          if (finalContent.trim().length > 0) {
            addMessage("assistant", finalContent)
          }
        }

      // Add completion step
      const completionStep = createProgressStep(
        "completion",
        "Task completed",
        "Successfully completed the requested task",
        "completed",
      )
      progressSteps.push(completionStep)

      // Emit final progress immediately for UI feedback
      emit({
        currentIteration: iteration,
        maxIterations,
        steps: progressSteps.slice(-3),
        isComplete: true,
        finalContent,
        conversationHistory: formatConversationForProgress(conversationHistory),
      })

      // Generate final completion summary (if dual-model enabled)
      // Await and emit follow-up to ensure the final summary is included
      if (isSummarizationEnabled()) {
        const lastToolCalls = conversationHistory
          .filter(m => m.toolCalls && m.toolCalls.length > 0)
          .flatMap(m => m.toolCalls || [])
          .slice(-5)
        const lastToolResults = conversationHistory
          .filter(m => m.toolResults && m.toolResults.length > 0)
          .flatMap(m => m.toolResults || [])
          .slice(-5)

        try {
          const completionSummary = await generateStepSummary(
            iteration,
            lastToolCalls,
            lastToolResults,
            finalContent,
            true, // isCompletion: this is the final completion step
          )

          // If a summary was generated, emit a follow-up progress update
          // to ensure the UI receives the completion summary
          if (completionSummary) {
            emit({
              currentIteration: iteration,
              maxIterations,
              steps: progressSteps.slice(-3),
              isComplete: true,
              finalContent,
              conversationHistory: formatConversationForProgress(conversationHistory),
            })
          }
        } catch (err) {
          if (isDebugLLM()) {
            logLLM("[Dual-Model] Completion summarization error:", err)
          }
        }
      }

      break
    }

    // Handle no-op iterations (no tool calls and no explicit completion)
    // Fix for https://github.com/aj47/nvidia-control-center/issues/443:
    // Only terminate when needsMoreWork is EXPLICITLY false, not when undefined.
    // When LLM returns plain text without tool calls, needsMoreWork will be undefined,
    // and we should nudge to either use tools or provide a complete answer.
    if (!hasToolCalls && !explicitlyComplete) {
      noOpCount++

      // Check if tools are available for this session (filtered for failures)
      // When tools are available, we give the LLM a chance to use them via nudges.
      // The nudge loop is bounded by MAX_NUDGES to prevent infinite loops.
      const hasToolsAvailable = activeTools.length > 0
      const contentText = llmResponse.content || ""

      // Check if tools have already been executed for THIS user prompt (current turn)
      // We only look at tool results AFTER currentPromptIndex to avoid treating a new request
      // as complete based on tool results from previous conversation turns
      // This fixes the infinite loop when LLM answers after tool execution but doesn't set needsMoreWork=false
      // Also check toolsExecutedInSession flag which survives context shrinking when drop_middle removes tool results
      const hasToolResultsInCurrentTurn = toolsExecutedInSession || conversationHistory.slice(currentPromptIndex + 1).some((e) => e.role === "tool")
      // When tools have been executed in this turn, accept any non-empty response (not just >= 10 chars)
      // Short answers like "1" or "3 sessions" are valid responses after tool execution
      const hasSubstantiveResponse = hasToolResultsInCurrentTurn
        ? contentText.trim().length > 0 && !isToolCallPlaceholder(contentText)
        : contentText.trim().length >= 10 && !isToolCallPlaceholder(contentText)

      const trimmedContent = contentText.trim()

      // IMPORTANT: If the LLM provides a substantive response without calling tools,
      // and indicates it's done (needsMoreWork !== true), accept it as complete ONLY if:
      // 1. There are no tools configured for this session (simple Q&A), OR
      // 2. The LLM explicitly set needsMoreWork to false (not just undefined)
      //
      // This prevents infinite loops for simple Q&A like "hi" while still allowing
      // nudge logic to push the LLM to use tools when tools are available and
      // needsMoreWork is undefined (plain text response without explicit completion).
      //
      // NOTE: hasToolsAvailable checks if tools are configured, not whether they're relevant
      // to the current prompt. However, if the LLM explicitly sets needsMoreWork=false, that
      // override takes precedence regardless of tool availability. This allows pure Q&A prompts
      // to complete immediately when the LLM signals completion, even in sessions with tools.
      //
      // EXCEPTION: If tools were executed in this turn, we should run verification (handled below).
      const hasAnyResponse = trimmedContent.length > 0 && !isToolCallPlaceholder(contentText)
      const shouldExitWithoutNudge = hasAnyResponse &&
        llmResponse.needsMoreWork !== true &&
        !hasToolResultsInCurrentTurn &&
        // Exit immediately if: no tools available, OR LLM explicitly signaled completion (false)
        // When needsMoreWork is undefined (not explicitly set) and tools exist, we nudge
        (!hasToolsAvailable || llmResponse.needsMoreWork === false)
      if (shouldExitWithoutNudge) {
        if (isDebugLLM()) {
          logLLM("Substantive response without tool calls - accepting as complete", {
            responseLength: trimmedContent.length,
            responsePreview: trimmedContent.substring(0, 100),
          })
        }
        finalContent = contentText
        addMessage("assistant", contentText)
        emit({
          currentIteration: iteration,
          maxIterations,
          steps: progressSteps.slice(-3),
          isComplete: true,
          finalContent,
          conversationHistory: formatConversationForProgress(conversationHistory),
        })
        break
      }

      // When agent claims it's done (needsMoreWork !== true) and has tool results + substantive response,
      // determine if the task is complete
      if (hasToolResultsInCurrentTurn && hasSubstantiveResponse && llmResponse.needsMoreWork !== true) {
        if (config.mcpVerifyCompletionEnabled) {
          // Use LLM verifier to determine if truly complete
          if (isDebugLLM()) {
            logLLM("Agent claims complete - running LLM verifier", {
              hasToolResults: hasToolResultsInCurrentTurn,
              responseLength: trimmedContent.length,
              responsePreview: trimmedContent.substring(0, 100),
            })
          }

          // Create a verification step for this path
          const verifyStep = createProgressStep(
            "thinking",
            "Verifying completion",
            "Checking that the user's request has been achieved",
            "in_progress",
          )
          progressSteps.push(verifyStep)
          emit({
            currentIteration: iteration,
            maxIterations,
            steps: progressSteps.slice(-3),
            isComplete: false,
            conversationHistory: formatConversationForProgress(conversationHistory),
          })

          // Use centralized verification handler (includes loop detection, retries, hard limits)
          const result = await runVerificationAndHandleResult(
            contentText,
            verifyStep,
            verificationFailCount,
            {
              checkRepeating: true,
              nudgeForToolUsage: true, // Nudge for tool usage since we have tools available
              currentPromptIndex, // Scope tool result checks to current turn
            }
          )
          verificationFailCount = result.newFailCount

          if (!result.shouldContinue) {
            // Verification passed or hard limit reached
            if (isDebugLLM()) {
              logLLM("Verifier confirmed completion", { isComplete: result.isComplete })
            }
            finalContent = contentText
            addMessage("assistant", contentText)
            emit({
              currentIteration: iteration,
              maxIterations,
              steps: progressSteps.slice(-3),
              isComplete: true,
              finalContent,
              conversationHistory: formatConversationForProgress(conversationHistory),
            })
            break
          } else {
            // Verifier says not complete - continue agent loop
            // The centralized handler already added the nudge message
            if (isDebugLLM()) {
              logLLM("Verifier says not complete - continuing agent loop", {
                responsePreview: trimmedContent.substring(0, 100),
                failCount: verificationFailCount,
              })
            }
            // Add the partial response to history and continue
            if (trimmedContent.length > 0) {
              addMessage("assistant", contentText)
            }
            noOpCount = 0
            continue
          }
        } else {
          // Verification is disabled - complete directly when tools executed + substantive response
          // This preserves the "tools executed + substantive response" completion behavior from #443
          if (isDebugLLM()) {
            logLLM("Completing without verification (disabled) - tools executed with substantive response", {
              hasToolResults: hasToolResultsInCurrentTurn,
              responseLength: trimmedContent.length,
              responsePreview: trimmedContent.substring(0, 100),
            })
          }
          finalContent = contentText
          addMessage("assistant", contentText)
          emit({
            currentIteration: iteration,
            maxIterations,
            steps: progressSteps.slice(-3),
            isComplete: true,
            finalContent,
            conversationHistory: formatConversationForProgress(conversationHistory),
          })
          break
        }
      }

      // Nudge the model to either use tools or provide a complete answer.
      // Only nudge when verification is enabled - when disabled, trust the LLM's decision.
      // When tools are available, nudge immediately (after 1 no-op).
      // When no tools are configured (simple Q&A), allow 2 no-ops before nudging,
      // giving the LLM a chance to self-correct.
      //
      // IMPORTANT: Track total nudges to prevent infinite loops. After MAX_NUDGES,
      // accept the current response as complete rather than nudging forever.
      //
      // EXCEPTION: If the model explicitly sets needsMoreWork=true, skip nudging entirely
      // and let it continue its multi-step answer naturally.
      if (config.mcpVerifyCompletionEnabled && llmResponse.needsMoreWork !== true && (noOpCount >= 2 || (hasToolsAvailable && noOpCount >= 1))) {
        // Check if we've exceeded max nudges - if so, accept the response as complete
        if (totalNudgeCount >= MAX_NUDGES) {
          const hasValidContent = contentText.trim().length > 0 && !isToolCallPlaceholder(contentText)
          if (isDebugLLM()) {
            logLLM("Max nudges reached - accepting response as complete", {
              totalNudgeCount,
              MAX_NUDGES,
              hasValidContent,
              responseLength: contentText.trim().length,
              responsePreview: contentText.trim().substring(0, 100),
            })
          }
          // Only use contentText if it's non-empty and not a placeholder
          // Otherwise provide a fallback message to avoid empty completion
          if (hasValidContent) {
            finalContent = contentText
            addMessage("assistant", contentText)
          } else {
            finalContent = "I was unable to complete the request. Please try rephrasing your question or provide more details."
            addMessage("assistant", finalContent)
          }
          emit({
            currentIteration: iteration,
            maxIterations,
            steps: progressSteps.slice(-3),
            isComplete: true,
            finalContent,
            conversationHistory: formatConversationForProgress(conversationHistory),
          })
          break
        }

        // Add nudge to push the agent forward
        // Only add assistant message if non-empty and not a placeholder to avoid blank entries
        if (contentText.trim().length > 0 && !isToolCallPlaceholder(contentText)) {
          addMessage("assistant", contentText)
        }

        const nudgeMessage = hasToolsAvailable
          ? "You have relevant tools available for this request. Please either call the tools directly using the native function calling interface, or provide a complete answer if the task cannot be accomplished with the available tools."
          : "Please provide a complete answer to the request. If you need to use tools, call them directly using the native function calling interface."

        addMessage("user", nudgeMessage)

        noOpCount = 0 // Reset counter after nudge
        totalNudgeCount++ // Track total nudges to prevent infinite loops
        if (isDebugLLM()) {
          logLLM("Nudging LLM for tool usage or complete answer", {
            totalNudgeCount,
            MAX_NUDGES,
            hasToolsAvailable,
          })
        }
        continue
      }

      // Handle needsMoreWork=true: the model explicitly wants to continue its multi-step answer.
      // This applies regardless of verification setting - respect the model's explicit signal.
      if (llmResponse.needsMoreWork === true) {
        if (isDebugLLM()) {
          logLLM("Model explicitly set needsMoreWork=true - continuing loop", {
            mcpVerifyCompletionEnabled: config.mcpVerifyCompletionEnabled,
            responseLength: contentText.trim().length,
            responsePreview: contentText.trim().substring(0, 100),
          })
        }
        // Add the partial response to history if non-empty
        if (contentText.trim().length > 0 && !isToolCallPlaceholder(contentText)) {
          addMessage("assistant", contentText)
        }
        noOpCount = 0 // Reset since the LLM explicitly signaled it needs more work
        // Reset nudge count since the model is making explicit progress - this allows
        // nudging to work per "stuck segment" rather than globally across the run.
        totalNudgeCount = 0
        continue
      }

      // When verification is disabled, handle text-only responses:
      // Accept the response as complete since we've already handled needsMoreWork=true above.
      // This prevents infinite loops when mcpVerifyCompletionEnabled is false.
      if (!config.mcpVerifyCompletionEnabled) {
        // Accept text-only response as complete
        const hasValidContent = contentText.trim().length > 0 && !isToolCallPlaceholder(contentText)
        if (isDebugLLM()) {
          logLLM("Verification disabled - accepting text-only response as complete", {
            hasValidContent,
            needsMoreWork: llmResponse.needsMoreWork,
            responseLength: contentText.trim().length,
            responsePreview: contentText.trim().substring(0, 100),
          })
        }
        if (hasValidContent) {
          finalContent = contentText
          addMessage("assistant", contentText)
        } else {
          // Provide a fallback message if no valid content
          finalContent = "I was unable to complete the request. Please try rephrasing your question or provide more details."
          addMessage("assistant", finalContent)
        }
        emit({
          currentIteration: iteration,
          maxIterations,
          steps: progressSteps.slice(-3),
          isComplete: true,
          finalContent,
          conversationHistory: formatConversationForProgress(conversationHistory),
        })
        break
      }
    } else {
      // Reset no-op counter when tools are called
      noOpCount = 0
      // Reset nudge count when tools are actually being used - this allows
      // nudging to work per "stuck segment" rather than globally across the run.
      // If the agent gets stuck again later, it should have a fresh nudge budget.
      totalNudgeCount = 0
    }

    // Execute tool calls with enhanced error handling
    const toolResults: MCPToolResult[] = []
    const failedTools: string[] = []

    // Add assistant response with tool calls to conversation history BEFORE executing tools
    // This ensures the tool call request is visible immediately in the UI
    addMessage("assistant", llmResponse.content || "", llmResponse.toolCalls || [])

    // Emit progress update to show tool calls immediately
    emit({
      currentIteration: iteration,
      maxIterations,
      steps: progressSteps.slice(-3),
      isComplete: false,
      conversationHistory: formatConversationForProgress(conversationHistory),
    })

    // Apply intelligent tool result processing to all queries to prevent context overflow

    // Check for stop signal before starting tool execution
    if (agentSessionStateManager.shouldStopSession(currentSessionId)) {
      logLLM(`Agent session ${currentSessionId} stopped before tool execution`)
      const killNote = "\n\n(Agent mode was stopped by emergency kill switch)"
      const finalOutput = (finalContent || "") + killNote
      conversationHistory.push({ role: "assistant", content: finalOutput, timestamp: Date.now() })
      emit({
        currentIteration: iteration,
        maxIterations,
        steps: progressSteps.slice(-3),
        isComplete: true,
        finalContent: finalOutput,
        conversationHistory: formatConversationForProgress(conversationHistory),
      })
      wasAborted = true
      break
    }

    // Determine execution mode: parallel or sequential
    // Sequential execution is used when config mcpParallelToolExecution is set to false
    // Default is parallel execution when multiple tools are called
    const forceSequential = config.mcpParallelToolExecution === false
    const useParallelExecution = !forceSequential && toolCallsArray.length > 1

    if (useParallelExecution) {
      // PARALLEL EXECUTION: Execute all tool calls concurrently
      if (isDebugTools()) {
        logTools(`Executing ${toolCallsArray.length} tool calls in parallel`, toolCallsArray.map(t => t.name))
      }

      // Create progress steps for all tools upfront
      // Use array index as key to avoid collisions when same tool is called with identical args
      const toolCallSteps: AgentProgressStep[] = []
      for (const toolCall of toolCallsArray) {
        const toolCallStep = createProgressStep(
          "tool_call",
          `Executing ${toolCall.name}`,
          `Running tool with arguments: ${JSON.stringify(toolCall.arguments)}`,
          "in_progress",
        )
        toolCallStep.toolCall = {
          name: toolCall.name,
          arguments: toolCall.arguments,
        }
        progressSteps.push(toolCallStep)
        toolCallSteps.push(toolCallStep)
      }

      // Emit progress showing all tools starting in parallel
      emit({
        currentIteration: iteration,
        maxIterations,
        steps: progressSteps.slice(-Math.min(toolCallsArray.length * 2, 6)),
        isComplete: false,
        conversationHistory: formatConversationForProgress(conversationHistory),
      })

      // Execute all tools in parallel
      const executionPromises = toolCallsArray.map(async (toolCall, index) => {
        const toolCallStep = toolCallSteps[index]

        const onToolProgress = (message: string) => {
          toolCallStep.description = message
          emit({
            currentIteration: iteration,
            maxIterations,
            steps: progressSteps.slice(-Math.min(toolCallsArray.length * 2, 6)),
            isComplete: false,
            conversationHistory: formatConversationForProgress(conversationHistory),
          })
        }

        const execResult = await executeToolWithRetries(
          toolCall,
          executeToolCall,
          currentSessionId,
          onToolProgress,
          2, // maxRetries
        )

        // Update the progress step with the result
        toolCallStep.status = execResult.result.isError ? "error" : "completed"
        toolCallStep.toolResult = {
          success: !execResult.result.isError,
          content: execResult.result.content.map((c) => c.text).join("\n"),
          error: execResult.result.isError
            ? execResult.result.content.map((c) => c.text).join("\n")
            : undefined,
        }

        // Add tool result step
        const toolResultStep = createProgressStep(
          "tool_result",
          `${toolCall.name} ${execResult.result.isError ? "failed" : "completed"}`,
          execResult.result.isError
            ? `Tool execution failed${execResult.retryCount > 0 ? ` after ${execResult.retryCount} retries` : ""}`
            : "Tool executed successfully",
          execResult.result.isError ? "error" : "completed",
        )
        toolResultStep.toolResult = toolCallStep.toolResult
        progressSteps.push(toolResultStep)

        return execResult
      })

      // Wait for all tools to complete
      const executionResults = await Promise.all(executionPromises)

      // Check if any tool was cancelled by kill switch
      const anyCancelled = executionResults.some(r => r.cancelledByKill)
      if (anyCancelled) {
        const killNote = "\n\n(Agent mode was stopped by emergency kill switch)"
        const finalOutput = (finalContent || "") + killNote
        conversationHistory.push({ role: "assistant", content: finalOutput, timestamp: Date.now() })
        emit({
          currentIteration: iteration,
          maxIterations,
          steps: progressSteps.slice(-Math.min(toolCallsArray.length * 2, 6)),
          isComplete: true,
          finalContent: finalOutput,
          conversationHistory: formatConversationForProgress(conversationHistory),
        })
        wasAborted = true
        break
      }

      // Collect results in order
      for (const execResult of executionResults) {
        toolResults.push(execResult.result)
        toolsExecutedInSession = true
        if (execResult.result.isError) {
          failedTools.push(execResult.toolCall.name)
        }
      }

      // Emit final progress for parallel execution
      emit({
        currentIteration: iteration,
        maxIterations,
        steps: progressSteps.slice(-Math.min(toolCallsArray.length * 2, 6)),
        isComplete: false,
        conversationHistory: formatConversationForProgress(conversationHistory),
      })
    } else {
      // SEQUENTIAL EXECUTION: Execute tool calls one at a time
      if (isDebugTools()) {
        const reason = toolCallsArray.length <= 1
          ? "Single tool call"
          : "Config disabled parallel execution"
        logTools(`Executing ${toolCallsArray.length} tool calls sequentially - ${reason}`, toolCallsArray.map(t => t.name))
      }
      for (const [, toolCall] of toolCallsArray.entries()) {
        if (isDebugTools()) {
          logTools("Executing planned tool call", toolCall)
        }
        // Check for stop signal before executing each tool
        if (agentSessionStateManager.shouldStopSession(currentSessionId)) {
          logLLM(`Agent session ${currentSessionId} stopped during tool execution`)
          const killNote = "\n\n(Agent mode was stopped by emergency kill switch)"
          const finalOutput = (finalContent || "") + killNote
          conversationHistory.push({ role: "assistant", content: finalOutput, timestamp: Date.now() })
          emit({
            currentIteration: iteration,
            maxIterations,
            steps: progressSteps.slice(-3),
            isComplete: true,
            finalContent: finalOutput,
            conversationHistory: formatConversationForProgress(conversationHistory),
          })
          wasAborted = true
          break
        }

        // Add tool call step
        const toolCallStep = createProgressStep(
          "tool_call",
          `Executing ${toolCall.name}`,
          `Running tool with arguments: ${JSON.stringify(toolCall.arguments)}`,
          "in_progress",
        )
        toolCallStep.toolCall = {
          name: toolCall.name,
          arguments: toolCall.arguments,
        }
        progressSteps.push(toolCallStep)

        // Emit progress update
        emit({
          currentIteration: iteration,
          maxIterations,
          steps: progressSteps.slice(-3),
          isComplete: false,
          conversationHistory: formatConversationForProgress(conversationHistory),
        })

        // Create progress callback to update tool execution step
        const onToolProgress = (message: string) => {
          toolCallStep.description = message
          emit({
            currentIteration: iteration,
            maxIterations,
            steps: progressSteps.slice(-3),
            isComplete: false,
            conversationHistory: formatConversationForProgress(conversationHistory),
          })
        }

        const execResult = await executeToolWithRetries(
          toolCall,
          executeToolCall,
          currentSessionId,
          onToolProgress,
          2, // maxRetries
        )

        if (execResult.cancelledByKill) {
          // Mark step and emit final progress, then break out of tool loop
          toolCallStep.status = "error"
          toolCallStep.toolResult = {
            success: false,
            content: "Tool execution cancelled by emergency kill switch",
            error: "Cancelled by emergency kill switch",
          }
          const toolResultStep = createProgressStep(
            "tool_result",
            `${toolCall.name} cancelled`,
            "Tool execution cancelled by emergency kill switch",
            "error",
          )
          toolResultStep.toolResult = toolCallStep.toolResult
          progressSteps.push(toolResultStep)
          const killNote = "\n\n(Agent mode was stopped by emergency kill switch)"
          const finalOutput = (finalContent || "") + killNote
          conversationHistory.push({ role: "assistant", content: finalOutput, timestamp: Date.now() })
          emit({
            currentIteration: iteration,
            maxIterations,
            steps: progressSteps.slice(-3),
            isComplete: true,
            finalContent: finalOutput,
            conversationHistory: formatConversationForProgress(conversationHistory),
          })
          wasAborted = true
          break
        }

        toolResults.push(execResult.result)
        toolsExecutedInSession = true

        // Track failed tools for better error reporting
        if (execResult.result.isError) {
          failedTools.push(toolCall.name)
        }

        // Update tool call step with result
        toolCallStep.status = execResult.result.isError ? "error" : "completed"
        toolCallStep.toolResult = {
          success: !execResult.result.isError,
          content: execResult.result.content.map((c) => c.text).join("\n"),
          error: execResult.result.isError
            ? execResult.result.content.map((c) => c.text).join("\n")
            : undefined,
        }

        // Add tool result step with enhanced error information
        const toolResultStep = createProgressStep(
          "tool_result",
          `${toolCall.name} ${execResult.result.isError ? "failed" : "completed"}`,
          execResult.result.isError
            ? `Tool execution failed${execResult.retryCount > 0 ? ` after ${execResult.retryCount} retries` : ""}`
            : "Tool executed successfully",
          execResult.result.isError ? "error" : "completed",
        )
        toolResultStep.toolResult = toolCallStep.toolResult
        progressSteps.push(toolResultStep)

        // Emit progress update
        emit({
          currentIteration: iteration,
          maxIterations,
          steps: progressSteps.slice(-3),
          isComplete: false,
          conversationHistory: formatConversationForProgress(conversationHistory),
        })
      }
    }

    // If stop was requested during tool execution, exit the agent loop now
    if (agentSessionStateManager.shouldStopSession(currentSessionId)) {
      // Emit final progress with complete status
      const killNote = "\n\n(Agent mode was stopped by emergency kill switch)"
      const finalOutput = (finalContent || "") + killNote
      addMessage("assistant", finalOutput)
      emit({
        currentIteration: iteration,
        maxIterations,
        steps: progressSteps.slice(-3),
        isComplete: true,
        finalContent: finalOutput,
        conversationHistory: formatConversationForProgress(conversationHistory),
      })
      wasAborted = true
      break
    }


    // Note: Assistant response with tool calls was already added before tool execution
    // This ensures the tool call request is visible immediately in the UI

    // Keep tool results intact for full visibility in UI
    // The UI will handle display and truncation as needed
    const processedToolResults = toolResults

    // Always add a tool message if any tools were executed, even if results are empty
    // This ensures the verifier sees tool execution evidence in conversationHistory
    if (processedToolResults.length > 0) {
      // For each result, use "[No output]" if the content is empty and not an error
      const resultsWithPlaceholders = processedToolResults.map((result) => {
        const contentText = result.content?.map((c) => c.text).join("").trim() || ""
        if (!result.isError && contentText.length === 0) {
          return {
            ...result,
            content: [{ type: "text" as const, text: "[No output]" }],
          }
        }
        return result
      })

      // Format tool results with tool name prefix for better context preservation
      // Format: [toolName] content... or [toolName] ERROR: content...
      const toolResultsText = resultsWithPlaceholders
        .map((result, i) => {
          const toolName = toolCallsArray[i]?.name || 'unknown'
          const content = result.content.map((c) => c.text).join("\n")
          const prefix = result.isError ? `[${toolName}] ERROR: ` : `[${toolName}] `
          return `${prefix}${content}`
        })
        .join("\n\n")

      addMessage("tool", toolResultsText, undefined, resultsWithPlaceholders)

      // Emit progress update immediately after adding tool results so UI shows them
      emit({
        currentIteration: iteration,
        maxIterations,
        steps: progressSteps.slice(-3),
        isComplete: false,
        conversationHistory: formatConversationForProgress(conversationHistory),
      })
    }

    // Generate step summary after tool execution (if dual-model enabled)
    // Fire-and-forget: summaries are for UI display, not needed for agent's next decision
    generateStepSummary(
      iteration,
      toolCallsArray,
      toolResults,
      llmResponse.content || undefined,
    ).catch(err => {
      if (isDebugLLM()) {
        logLLM("[Dual-Model] Background summarization error:", err)
      }
    })

    // Enhanced completion detection with better error handling
    const hasErrors = toolResults.some((result) => result.isError)
    const allToolsSuccessful = toolResults.length > 0 && !hasErrors

    if (hasErrors) {
      // Enhanced error analysis and recovery suggestions
      const errorAnalysis = analyzeToolErrors(toolResults)

      // Track per-tool failures
      for (let i = 0; i < toolResults.length; i++) {
        const result = toolResults[i]
        if (result.isError) {
          // Get the tool name from toolCallsArray by index
          const toolName = toolCallsArray[i]?.name || "unknown"
          const currentCount = toolFailureCount.get(toolName) || 0
          toolFailureCount.set(toolName, currentCount + 1)

          if (currentCount + 1 >= MAX_TOOL_FAILURES) {
            logLLM(`âš ï¸ Tool "${toolName}" has failed ${MAX_TOOL_FAILURES} times - will be excluded`)
          }
        }
      }

      // Check for unrecoverable errors that should trigger early completion
      const hasUnrecoverableError = errorAnalysis.errorTypes?.some(
        type => type === "permissions" || type === "authentication"
      )
      if (hasUnrecoverableError) {
        // Build list of tools that failed with unrecoverable errors in THIS batch only
        // (not all historical failures from toolFailureCount, which could mislead the model)
        const currentUnrecoverableTools: string[] = []
        for (let i = 0; i < toolResults.length; i++) {
          const result = toolResults[i]
          if (result.isError) {
            const errorText = result.content.map((c) => c.text).join(" ").toLowerCase()
            if (errorText.includes("permission") || errorText.includes("access") ||
                errorText.includes("denied") || errorText.includes("authentication") ||
                errorText.includes("unauthorized") || errorText.includes("forbidden")) {
              const toolName = toolCallsArray[i]?.name || "unknown"
              currentUnrecoverableTools.push(toolName)
            }
          }
        }

        if (currentUnrecoverableTools.length > 0) {
          const failedToolNames = currentUnrecoverableTools.join(", ")
          logLLM(`âš ï¸ Unrecoverable errors detected for tools: ${failedToolNames}`)
          // Add note to conversation so LLM knows to wrap up
          conversationHistory.push({
            role: "user",
            content: `Note: Some tools (${failedToolNames}) have unrecoverable errors (permissions/authentication). Please complete what you can or explain what cannot be done.`,
            timestamp: Date.now()
          })
        }
      }

      // Add clean error summary to conversation history for LLM context
      const errorSummary = failedTools
        .map((toolName, idx) => {
          const failedResult = toolResults.filter((r) => r.isError)[idx]
          const rawError = failedResult?.content.map((c) => c.text).join(" ") || "Unknown error"
          const cleanedError = cleanErrorMessage(rawError)
          const failureCount = toolFailureCount.get(toolName) || 1
          return `TOOL FAILED: ${toolName} (attempt ${failureCount}/${MAX_TOOL_FAILURES})\nError: ${cleanedError}`
        })
        .join("\n\n")

      conversationHistory.push({
        role: "tool",
        content: errorSummary,
        timestamp: Date.now(),
      })
    }

    // Check if agent indicated it was done after executing tools
    const agentIndicatedDone = llmResponse.needsMoreWork === false

    if (agentIndicatedDone && allToolsSuccessful) {
      // Agent indicated completion, but we need to ensure we have a proper summary
      // If the last assistant content was just tool calls, prompt for a summary
      const lastAssistantContent = llmResponse.content || ""

      // Check if the last assistant message was primarily tool calls without much explanation
      const hasToolCalls = llmResponse.toolCalls && llmResponse.toolCalls.length > 0
      const hasMinimalContent = lastAssistantContent.trim().length < 50

      if (hasToolCalls && (hasMinimalContent || !lastAssistantContent.trim())) {
        // The agent just made tool calls without providing a summary
        // Prompt the agent to provide a concise summary of what was accomplished
        const summaryPrompt = "Please provide a concise summary of what you just accomplished with the tool calls. Focus on the key results and outcomes for the user."

        conversationHistory.push({
          role: "user",
          content: summaryPrompt,
          timestamp: Date.now(),
        })

        // Create a summary request step
        const summaryStep = createProgressStep(
          "thinking",
          "Generating summary",
          "Requesting final summary of completed actions",
          "in_progress",
        )
        progressSteps.push(summaryStep)

        // Emit progress update for summary request
        emit({
          currentIteration: iteration,
          maxIterations,
          steps: progressSteps.slice(-3),
          isComplete: false,
          conversationHistory: formatConversationForProgress(conversationHistory),
        })

        // Get the summary from the agent
        const contextAwarePrompt = constructSystemPrompt(
          uniqueAvailableTools,
          agentModeGuidelines, // Use session-bound guidelines
          true, // isAgentMode
          undefined, // relevantTools
          customSystemPrompt, // Use session-bound custom system prompt
          skillsInstructions, // agent skills instructions
          personaProperties, // dynamic persona properties
          relevantMemories, // memories from previous sessions
        )

        const summaryMessages = [
          { role: "system" as const, content: contextAwarePrompt },
          ...mapConversationToMessages(),
        ]

        const { messages: shrunkSummaryMessages, estTokensAfter: summaryEstTokens, maxTokens: summaryMaxTokens } = await shrinkMessagesForLLM({
          messages: summaryMessages as any,
          availableTools: uniqueAvailableTools,
          relevantTools: undefined,
          isAgentMode: true,
          sessionId: currentSessionId,
          onSummarizationProgress: (current, total) => {
            summaryStep.description = `Summarizing for summary generation (${current}/${total})`
            emit({
              currentIteration: iteration,
              maxIterations,
              steps: progressSteps.slice(-3),
              isComplete: false,
              conversationHistory: formatConversationForProgress(conversationHistory),
            })
          },
        })
        // Update context info for progress display
        contextInfoRef = { estTokens: summaryEstTokens, maxTokens: summaryMaxTokens }


        try {
          const summaryResponse = await makeLLMCall(shrunkSummaryMessages, config, onRetryProgress, undefined, currentSessionId)

          // Check if stop was requested during summary generation
          if (agentSessionStateManager.shouldStopSession(currentSessionId)) {
            logLLM(`Agent session ${currentSessionId} stopped during summary generation`)
            const killNote = "\n\n(Agent mode was stopped by emergency kill switch)"
            const finalOutput = (finalContent || "") + killNote
            conversationHistory.push({ role: "assistant", content: finalOutput, timestamp: Date.now() })
            emit({
              currentIteration: iteration,
              maxIterations,
              steps: progressSteps.slice(-3),
              isComplete: true,
              finalContent: finalOutput,
              conversationHistory: formatConversationForProgress(conversationHistory),
            })
            wasAborted = true
            break
          }

          // Update summary step with the response
          summaryStep.status = "completed"
          summaryStep.llmContent = summaryResponse.content || ""
          summaryStep.title = "Summary provided"
          summaryStep.description = summaryResponse.content && summaryResponse.content.length > 100
            ? summaryResponse.content.substring(0, 100) + "..."
            : summaryResponse.content || "Summary generated"

          // Use the summary as final content
          finalContent = summaryResponse.content || lastAssistantContent

          // Add the summary to conversation history
          conversationHistory.push({
            role: "assistant",
            content: finalContent,
            timestamp: Date.now(),
          })
        } catch (error) {
          // If summary generation fails, fall back to the original content
          logLLM("Failed to generate summary:", error)
          finalContent = lastAssistantContent || "Task completed successfully."
          summaryStep.status = "error"
          summaryStep.description = "Failed to generate summary, using fallback"

          conversationHistory.push({
            role: "assistant",
            content: finalContent,
            timestamp: Date.now(),
          })
        }
      } else {
        // Agent provided sufficient content, use it as final content
        finalContent = lastAssistantContent
      }


	      // Optional verification before completing after tools
	      // Track if we should skip post-verify summary (when agent is repeating itself or disabled)
	      let skipPostVerifySummary2 = config.mcpFinalSummaryEnabled === false

	      if (config.mcpVerifyCompletionEnabled) {
	        const verifyStep = createProgressStep(
	          "thinking",
	          "Verifying completion",
	          "Checking that the user's request has been achieved",
	          "in_progress",
	        )
	        progressSteps.push(verifyStep)
	        emit({
	          currentIteration: iteration,
	          maxIterations,
	          steps: progressSteps.slice(-3),
	          isComplete: false,
	          conversationHistory: formatConversationForProgress(conversationHistory),
	        })

	        const result = await runVerificationAndHandleResult(
	          finalContent,
	          verifyStep,
	          verificationFailCount,
	          { checkRepeating: true }
	        )
	        verificationFailCount = result.newFailCount
	        if (result.skipPostVerifySummary) {
	          skipPostVerifySummary2 = true
	        }

	        // Check if stop was requested during verification
	        if (agentSessionStateManager.shouldStopSession(currentSessionId)) {
	          logLLM(`Agent session ${currentSessionId} stopped during verification`)
	          const killNote = "\n\n(Agent mode was stopped by emergency kill switch)"
	          const finalOutput = (finalContent || "") + killNote
	          conversationHistory.push({ role: "assistant", content: finalOutput, timestamp: Date.now() })
	          emit({
	            currentIteration: iteration,
	            maxIterations,
	            steps: progressSteps.slice(-3),
	            isComplete: true,
	            finalContent: finalOutput,
	            conversationHistory: formatConversationForProgress(conversationHistory),
	          })
	          wasAborted = true
	          break
	        }

	        if (result.shouldContinue) {
	          noOpCount = 0
	          continue
	        }
	      }

        // Post-verify: produce a concise final summary for the user
        if (!skipPostVerifySummary2) {
          try {
            const result = await generatePostVerifySummary(finalContent, true, activeTools)
            if (result.stopped) {
              const killNote = "\n\n(Agent mode was stopped by emergency kill switch)"
              const finalOutput = (finalContent || "") + killNote
              conversationHistory.push({ role: "assistant", content: finalOutput, timestamp: Date.now() })
              emit({
                currentIteration: iteration,
                maxIterations,
                steps: progressSteps.slice(-3),
                isComplete: true,
                finalContent: finalOutput,
                conversationHistory: formatConversationForProgress(conversationHistory),
              })
              break
            }
            finalContent = result.content
            if (finalContent.trim().length > 0) {
              conversationHistory.push({ role: "assistant", content: finalContent, timestamp: Date.now() })
            }
          } catch (e) {
            // If summary generation fails, still add the existing finalContent to history
            // so the mobile client has the complete conversation
            if (finalContent.trim().length > 0) {
              conversationHistory.push({ role: "assistant", content: finalContent, timestamp: Date.now() })
            }
          }
        } else {
          // Even when skipping post-verify summary, ensure the final content is in history
          // This prevents intermediate messages from disappearing on mobile
          if (finalContent.trim().length > 0) {
            conversationHistory.push({ role: "assistant", content: finalContent, timestamp: Date.now() })
          }
        }


      // Add completion step
      const completionStep = createProgressStep(
        "completion",
        "Task completed",
        "Successfully completed the requested task with summary",
        "completed",
      )
      progressSteps.push(completionStep)

      // Emit final progress
      emit({
        currentIteration: iteration,
        maxIterations,
        steps: progressSteps.slice(-3),
        isComplete: true,
        finalContent,
        conversationHistory: formatConversationForProgress(conversationHistory),
      })

      break
    }

    // Continue iterating if needsMoreWork is true (explicitly set) or undefined (default behavior)
    // Only stop if needsMoreWork is explicitly false or we hit max iterations
    const shouldContinue = llmResponse.needsMoreWork !== false
    if (!shouldContinue) {
      // Agent explicitly indicated no more work needed
      const assistantContent = llmResponse.content || ""

      // Check if we just executed tools and need a summary
      const hasToolCalls = llmResponse.toolCalls && llmResponse.toolCalls.length > 0
      const hasMinimalContent = assistantContent.trim().length < 50

      if (hasToolCalls && (hasMinimalContent || !assistantContent.trim())) {
        // The agent just made tool calls without providing a summary
        // Prompt the agent to provide a concise summary of what was accomplished
        const summaryPrompt = "Please provide a concise summary of what you just accomplished with the tool calls. Focus on the key results and outcomes for the user."

        conversationHistory.push({
          role: "user",
          content: summaryPrompt,
          timestamp: Date.now(),
        })

        // Create a summary request step
        const summaryStep = createProgressStep(
          "thinking",
          "Generating summary",
          "Requesting final summary of completed actions",
          "in_progress",
        )
        progressSteps.push(summaryStep)

        // Emit progress update for summary request
        emit({
          currentIteration: iteration,
          maxIterations,
          steps: progressSteps.slice(-3),
          isComplete: false,
          conversationHistory: formatConversationForProgress(conversationHistory),
        })

        // Get the summary from the agent
        const contextAwarePrompt = constructSystemPrompt(
          uniqueAvailableTools,
          agentModeGuidelines, // Use session-bound guidelines
          true, // isAgentMode
          undefined, // relevantTools
          customSystemPrompt, // Use session-bound custom system prompt
          skillsInstructions, // agent skills instructions
          personaProperties, // dynamic persona properties
          relevantMemories, // memories from previous sessions
        )

        const summaryMessages = [
          { role: "system" as const, content: contextAwarePrompt },
          ...mapConversationToMessages(),
        ]

        const { messages: shrunkSummaryMessages, estTokensAfter: summaryEstTokens2, maxTokens: summaryMaxTokens2 } = await shrinkMessagesForLLM({
          messages: summaryMessages as any,
          availableTools: uniqueAvailableTools,
          relevantTools: undefined,
          isAgentMode: true,
          sessionId: currentSessionId,
          onSummarizationProgress: (current, total) => {
            summaryStep.description = `Summarizing for summary generation (${current}/${total})`
            emit({
              currentIteration: iteration,
              maxIterations,
              steps: progressSteps.slice(-3),
              isComplete: false,
              conversationHistory: formatConversationForProgress(conversationHistory),
            })
          },
        })
        // Update context info for progress display
        contextInfoRef = { estTokens: summaryEstTokens2, maxTokens: summaryMaxTokens2 }


        try {
          const summaryResponse = await makeLLMCall(shrunkSummaryMessages, config, onRetryProgress, undefined, currentSessionId)

          // Update summary step with the response
          summaryStep.status = "completed"
          summaryStep.llmContent = summaryResponse.content || ""
          summaryStep.title = "Summary provided"
          summaryStep.description = summaryResponse.content && summaryResponse.content.length > 100
            ? summaryResponse.content.substring(0, 100) + "..."
            : summaryResponse.content || "Summary generated"

          // Use the summary as final content
          finalContent = summaryResponse.content || assistantContent

          // Add the summary to conversation history
          conversationHistory.push({
            role: "assistant",
            content: finalContent,
            timestamp: Date.now(),
          })
        } catch (error) {
          // If summary generation fails, fall back to the original content
          logLLM("Failed to generate summary:", error)
          finalContent = assistantContent || "Task completed successfully."
          summaryStep.status = "error"
          summaryStep.description = "Failed to generate summary, using fallback"

          conversationHistory.push({
            role: "assistant",
            content: finalContent,
            timestamp: Date.now(),
          })
        }

        // NOTE: Removed duplicate "Post-verify summary" block that was causing empty content issues.
        // The summary was already generated above - making another LLM call here would cause
        // the model to return empty content since it already provided a complete response.

        // If there are actionable tools and we haven't executed any tools yet,
        // skip verification and force the model to produce structured toolCalls instead of intent-only text.
        const hasAnyToolResultsSoFar = conversationHistory.some((e) => e.role === "tool")
        // Use activeTools (filtered for failures) to avoid nudging for excluded tools
        const hasActionableTools = activeTools.length > 0
        if (hasActionableTools && !hasAnyToolResultsSoFar) {
          conversationHistory.push({
            role: "user",
            content:
              "Before verifying or completing: please use the available tools to actually perform the steps. Call them directly using the native function calling interface.",
            timestamp: Date.now(),
          })
          noOpCount = 0
          continue
        }
      } else {
        // Agent provided sufficient content, use it as final content
        finalContent = assistantContent
        conversationHistory.push({
          role: "assistant",
          content: finalContent,
          timestamp: Date.now(),
        })
      }


	      // Optional verification before completing (general stop condition)
	      if (config.mcpVerifyCompletionEnabled) {
	        const verifyStep = createProgressStep(
	          "thinking",
	          "Verifying completion",
	          "Checking that the user's request has been achieved",
	          "in_progress",
	        )
	        progressSteps.push(verifyStep)
	        emit({
	          currentIteration: iteration,
	          isComplete: false,
	          maxIterations,
	          steps: progressSteps.slice(-3),
	          conversationHistory: formatConversationForProgress(conversationHistory),
	        })

	        const result = await runVerificationAndHandleResult(
	          finalContent,
	          verifyStep,
	          verificationFailCount,
	          { checkRepeating: true }
	        )
	        verificationFailCount = result.newFailCount

	        if (result.shouldContinue) {
	          noOpCount = 0
	          continue
	        }
	      }

      const completionStep = createProgressStep(
        "completion",
        "Task completed",
        "Agent indicated no more work needed",
        "completed",
      )
      progressSteps.push(completionStep)

      emit({
        currentIteration: iteration,
        maxIterations,
        steps: progressSteps.slice(-3),
        isComplete: true,
        finalContent,
        conversationHistory: formatConversationForProgress(conversationHistory),
      })

      break
    }

    // Set final content to the latest assistant response (fallback)
    if (!finalContent) {
      finalContent = llmResponse.content || ""
    }
  }

  if (iteration >= maxIterations) {
    // Handle maximum iterations reached - always ensure we have a meaningful summary
    const hasRecentErrors = progressSteps
      .slice(-5)
      .some((step) => step.status === "error")

    // If we don't have final content, get the last assistant response or provide fallback
    if (!finalContent) {
      const lastAssistantMessage = conversationHistory
        .slice()
        .reverse()
        .find((msg) => msg.role === "assistant")

      if (lastAssistantMessage) {
        finalContent = lastAssistantMessage.content
      } else {
        // Provide a fallback summary
        finalContent = hasRecentErrors
          ? "Task was interrupted due to repeated tool failures. Please review the errors above and try again with alternative approaches."
          : "Task reached maximum iteration limit while still in progress. Some actions may have been completed successfully - please review the tool results above."
      }
    }

    // Add context about the termination reason
    const terminationNote = hasRecentErrors
      ? "\n\n(Note: Task incomplete due to repeated tool failures. Please try again or use alternative methods.)"
      : "\n\n(Note: Task may not be fully complete - reached maximum iteration limit. The agent was still working on the request.)"

    finalContent += terminationNote

    // Make sure the final message is added to conversation history
    const lastMessage = conversationHistory[conversationHistory.length - 1]
    if (
      !lastMessage ||
      lastMessage.role !== "assistant" ||
      lastMessage.content !== finalContent
    ) {
      conversationHistory.push({
        role: "assistant",
        content: finalContent,
        timestamp: Date.now(),
      })
    }

    // Add timeout completion step with better context
    const timeoutStep = createProgressStep(
      "completion",
      "Maximum iterations reached",
      hasRecentErrors
        ? "Task stopped due to repeated tool failures"
        : "Task stopped due to iteration limit",
      "error",
    )
    progressSteps.push(timeoutStep)

    // Emit final progress
    emit({
      currentIteration: iteration,
      maxIterations,
      steps: progressSteps.slice(-3),
      isComplete: true,
      finalContent,
      conversationHistory: formatConversationForProgress(conversationHistory),
    })
  }

    return {
      content: finalContent,
      conversationHistory,
      totalIterations: iteration,
    }
  } finally {
    // End Langfuse trace for this agent session if enabled
    // This is in a finally block to ensure traces are closed even on unexpected exceptions
    if (isLangfuseEnabled()) {
      endAgentTrace(currentSessionId, {
        output: finalContent,
        metadata: {
          totalIterations: iteration,
          wasAborted,
        },
      })
      // Flush to ensure trace is sent
      flushLangfuse().catch(() => {})
    }

    // Clean up session state at the end of agent processing
    agentSessionStateManager.cleanupSession(currentSessionId)
  }
}

async function makeLLMCall(
  messages: Array<{ role: string; content: string }>,
  config: any,
  onRetryProgress?: RetryProgressCallback,
  onStreamingUpdate?: StreamingCallback,
  sessionId?: string,
  tools?: MCPTool[],
): Promise<LLMToolCallResponse> {
  const chatProviderId = config.mcpToolsProviderId

  try {
    if (isDebugLLM()) {
      logLLM("=== LLM CALL START ===")
      logLLM("Messages â†’", {
        count: messages.length,
        totalChars: messages.reduce((sum, msg) => sum + msg.content.length, 0),
        messages: messages,
      })
      if (tools) {
        logLLM("Tools â†’", {
          count: tools.length,
          names: tools.map(t => t.name),
        })
      }
    }

    // If streaming callback is provided, use streaming
    // Note: Streaming is only for display purposes - we still need the full response for tool calls
    if (onStreamingUpdate) {
      // Create abort controller for streaming - we'll abort when structured call completes
      const streamingAbortController = new AbortController()

      // Register with session manager so user-initiated stop will also cancel streaming
      if (sessionId) {
        agentSessionStateManager.registerAbortController(sessionId, streamingAbortController)
      }

      // Track whether streaming should be aborted (when structured call completes)
      // This prevents late streaming updates from appearing after the response is ready
      let streamingAborted = false

      // Track the last accumulated streaming content to use as the final text
      // This ensures the user sees the same content they watched stream in
      let lastStreamedContent = ""

      // Track whether streaming failed - if so, we should not use partial/stale content
      // to overwrite the full structured response
      let streamingFailed = false

      // Wrap the callback to ignore updates after the structured call completes
      // and track the accumulated content for consistency
      const wrappedOnStreamingUpdate = (chunk: string, accumulated: string) => {
        if (!streamingAborted) {
          lastStreamedContent = accumulated
          onStreamingUpdate(chunk, accumulated)
        }
      }

      // Start a parallel streaming call for real-time display
      // This runs alongside the structured call to provide live feedback
      const streamingPromise = makeLLMCallWithStreaming(
        messages,
        wrappedOnStreamingUpdate,
        chatProviderId,
        sessionId,
        streamingAbortController,
      ).catch(err => {
        // Streaming errors are non-fatal - we still have the structured call
        // Mark streaming as failed so we don't use partial/stale content
        streamingFailed = true
        if (isDebugLLM()) {
          logLLM("Streaming call failed (non-fatal):", err)
        }
        return null
      })

      // Make the structured call for the actual response
      // Wrap in try/finally to ensure streaming is cleaned up even if the call fails
      let result: LLMToolCallResponse
      try {
        result = await makeLLMCallWithFetch(messages, chatProviderId, onRetryProgress, sessionId, tools)
      } finally {
        // Signal streaming to stop IMMEDIATELY
        streamingAborted = true
        streamingAbortController.abort()

        // Wait briefly for streaming to acknowledge the abort (prevents race)
        // This ensures streaming callbacks don't fire after we return
        await Promise.race([
          streamingPromise,
          new Promise(resolve => setTimeout(resolve, 100))  // 100ms max wait
        ]).catch(() => {}) // Ignore any errors

        // Unregister after streaming has stopped
        if (sessionId) {
          agentSessionStateManager.unregisterAbortController(sessionId, streamingAbortController)
        }
      }

      // Use the streamed content for display consistency if:
      // 1. We have streamed content AND
      // 2. Streaming didn't fail (to avoid using partial/stale content) AND
      // 3. There are no tool calls (to maintain consistency between content and toolCalls)
      // This ensures what the user saw streaming is what they get at the end for text-only responses.
      // When tool calls are present, we keep the structured response content to maintain
      // consistency between content and toolCalls in the conversation history.
      // This prevents downstream agent logic from seeing mismatched text content and tool calls.
      const hasToolCalls = result.toolCalls && result.toolCalls.length > 0
      if (lastStreamedContent && !streamingFailed && !hasToolCalls) {
        result = {
          ...result,
          content: lastStreamedContent,
        }
      }

      if (isDebugLLM()) {
        logLLM("Response â†", result)
        logLLM("=== LLM CALL END ===")
      }
      return result
    }

    // Non-streaming path
    const result = await makeLLMCallWithFetch(messages, chatProviderId, onRetryProgress, sessionId, tools)
    if (isDebugLLM()) {
      logLLM("Response â†", result)
      logLLM("=== LLM CALL END ===")
    }
    return result
  } catch (error) {
    if (isDebugLLM()) {
      logLLM("LLM CALL ERROR:", error)
    }
    diagnosticsService.logError("llm", "Agent LLM call failed", error)
    throw error
  }
}
